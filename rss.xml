<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Obsidian Vault]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib/media/favicon.png</url><title>Obsidian Vault</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Fri, 03 Jan 2025 18:04:14 GMT</lastBuildDate><atom:link href="lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Fri, 03 Jan 2025 18:03:58 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[Alternating BFS]]></title><description><![CDATA[ 
 <br>Mark every vertex as unexplored. <br>Mark vertices in  as explored. <br>Insert  into a queue , make them the roots of . <br>While  is not empty, remove  from it.<br>
For every unexplored neighbour , mark it as explored and make it 's child.<br>
If  is matched by another vertex , then also mark  as explored and make it 's child, then add  to .<br>
 We add  to  to make sure it's even (defined in <a data-href="Alternating Forest" href="algorithms-ii-concepts/alternating-forest.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating Forest</a>).<br>
 No need to worry about  being explored as it would only get marked as so if a neighbour in  already was.<br>The computed forest is maximum for  (as defined in <a data-href="Alternating Forest" href="algorithms-ii-concepts/alternating-forest.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating Forest</a>)<br>From the above definition, the child edges of even vertices will always be unmatched while those of odd vertices will be matched. <br>But is  maximum (also defined in <a data-href="Alternating Forest" href="algorithms-ii-concepts/alternating-forest.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating Forest</a>)?<br> If a vertex  is in  then, again, by the above definition, it must be reachable from its root.<br> If there exists an alternating path between a vertex in  and , the consider the shortest alternating path between  and some vertex in , prove by induction on the size of this path.<br>
 If the size is , then , and so, it must belong to .<br>
 If the size is , then consider the vertex right before  in this path, call it . The path between  and  must also be the shortest alternating path between  and a vertex in . By induction hypothesis, .<br>
 If  and , and the graph is <a data-href="Bipartite" href="algorithms-ii-concepts/bipartite.html" class="internal-link" target="_self" rel="noopener nofollow">Bipartite</a>, it must be true that the path between  and  is even, and moreover, that  is also unmatched.<br>
This makes  even, and therefore, alternating BFS will discover  from  unless  is already in  If , then  is even and unmatched (again, because the graph is <a data-href="Bipartite" href="algorithms-ii-concepts/bipartite.html" class="internal-link" target="_self" rel="noopener nofollow">Bipartite</a>), and it will therefore get added to  alongside . ]]></description><link>algorithms-ii-algorithms/alternating-bfs.html</link><guid isPermaLink="false">Algorithms II Algorithms/Alternating BFS.md</guid><pubDate>Tue, 17 Dec 2024 21:16:20 GMT</pubDate></item><item><title><![CDATA[Bipartite Maximum-Cardinality Matching]]></title><description><![CDATA[ 
 <br>We construct an <a data-href="Alternating Forest" href="algorithms-ii-concepts/alternating-forest.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating Forest</a> using the <a data-href="Alternating BFS" href="algorithms-ii-algorithms/alternating-bfs.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating BFS</a> routine in  time.<br>We, then, inspect every vertex in  time, and if it happens to be a vertex in  that is unmatched, then we are guaranteed to have an alternating path between it and the root of its tree. Perform . <br>Since a <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> cannot have more than  edges, we do this  times, so total running time is  (?). <br>We can shortcut this running time by removing isolated vertices beforehand, yielding a  running time.]]></description><link>algorithms-ii-algorithms/bipartite-maximum-cardinality-matching.html</link><guid isPermaLink="false">Algorithms II Algorithms/Bipartite Maximum-Cardinality Matching.md</guid><pubDate>Sun, 15 Dec 2024 18:06:04 GMT</pubDate></item><item><title><![CDATA[Bipartite Maximum-Weight Matching]]></title><description><![CDATA[ 
 <br>This is the same as computing a minimum-weight <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> after negating all the edge weights. <br>But, we are not necessarily dealing with a <a data-href="Complete Bipartite Graph" href="algorithms-ii-concepts/complete-bipartite-graph.html" class="internal-link" target="_self" rel="noopener nofollow">Complete Bipartite Graph</a>, so we can't get a <a data-href="Perfect matching" href="algorithms-ii-concepts/perfect-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Perfect matching</a> unless we do it on an auxiliary graph , defined as the original graph  and an identical copy of it  with an edge whose weight is  between every edge and its twin. <br>Note that we can reduce the running time of <a data-href="The Hungarian Algorithm" href="algorithms-ii-algorithms/the-hungarian-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">The Hungarian Algorithm</a> to . <br>Claim: If we take every edge in the minimum-weight <a data-href="Perfect matching" href="algorithms-ii-concepts/perfect-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Perfect matching</a>  of  that actually exists in , we get a a minimum-weight <a data-href="Perfect matching" href="algorithms-ii-concepts/perfect-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Perfect matching</a>  of . <br>Proof:<br>Consider the minimum-weight <a data-href="Perfect matching" href="algorithms-ii-concepts/perfect-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Perfect matching</a>  of . <br>Then,  where  is the set of edges from  in , with the same relation applying for  and .<br>Let  be the minimum-weight <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of , we want to show that the weight of  is exactly the weight of . <br>First, observe that<br>
<br>Now, consider, for the sake of contradiction, that . This implies that<br>
Which is a contradiction because we can construct a <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of all the edges in  and their twins in  (and all the edges between vertices that are not in  and their twins) whose weight would be exactly . <br>Therefore, it can only be true that , and since we claimed that , it must be true that . ]]></description><link>algorithms-ii-algorithms/bipartite-maximum-weight-matching.html</link><guid isPermaLink="false">Algorithms II Algorithms/Bipartite Maximum-Weight Matching.md</guid><pubDate>Mon, 16 Dec 2024 04:33:17 GMT</pubDate></item><item><title><![CDATA[Bipartite Minimum-Weight Perfect Matching LP]]></title><description><![CDATA[ 
 <br>To be solved over a <a data-href="Complete Bipartite Graph" href="algorithms-ii-concepts/complete-bipartite-graph.html" class="internal-link" target="_self" rel="noopener nofollow">Complete Bipartite Graph</a>, we shall use <a data-href="Primal-Dual Schema" href="algorithms-ii-concepts/primal-dual-schema.html" class="internal-link" target="_self" rel="noopener nofollow">Primal-Dual Schema</a>. <br>Formulate an LP:<br>
<img alt="Pasted image 20241215133255.png" src="lib/media/pasted-image-20241215133255.png"><br>
And its <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a>:<br>
<img alt="Pasted image 20241215140330.png" src="lib/media/pasted-image-20241215140330.png"><br>
We maintain <a data-href="Complementary Slackness" href="algorithms-ii-concepts/complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a>: <br><img alt="Pasted image 20241215140551.png" src="lib/media/pasted-image-20241215140551.png"><br>Now, set  and  to make our <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> look nicer:<br>
<img alt="Pasted image 20241215151714.png" src="lib/media/pasted-image-20241215151714.png"><br>
Our <a data-href="Complementary Slackness" href="algorithms-ii-concepts/complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a> conditions then become<br>
<img alt="Pasted image 20241215151741.png" src="lib/media/pasted-image-20241215151741.png"><br> We can think of  as the potential of a vertex . The condition above tells us that we need to find a <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of the graph  (a feasible solution  for the primal), while maintaining that the potential of every edge in  is tight (should not exceed , if it does then, by <a data-href="Complementary Slackness" href="algorithms-ii-concepts/complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a>, , meaning that it is not included in the <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> ).  ]]></description><link>algorithms-ii-algorithms/bipartite-minimum-weight-perfect-matching-lp.html</link><guid isPermaLink="false">Algorithms II Algorithms/Bipartite Minimum-Weight Perfect Matching LP.md</guid><pubDate>Sun, 15 Dec 2024 22:11:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20241215133255.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241215133255.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Cluster Editing Branching]]></title><description><![CDATA[ 
 <br> For  to be a cluster graph, it has to have at least the following <br><img alt="Pasted image 20241216110120.png" src="lib/media/pasted-image-20241216110120.png"><br> Once we find this triple, we can either remove , remove , or add , giving us a <a data-href="Branching vector" href="algorithms-ii-concepts/branching-vector.html" class="internal-link" target="_self" rel="noopener nofollow">Branching vector</a> of .<br>
<img alt="Pasted image 20241216110252.png" src="lib/media/pasted-image-20241216110252.png"><br>
 This yields a <a data-href="Branching number" href="algorithms-ii-concepts/branching-number.html" class="internal-link" target="_self" rel="noopener nofollow">Branching number</a> of , and since we bound our depth by , a total running time of . ]]></description><link>algorithms-ii-algorithms/cluster-editing-branching.html</link><guid isPermaLink="false">Algorithms II Algorithms/Cluster Editing Branching.md</guid><pubDate>Mon, 16 Dec 2024 17:06:36 GMT</pubDate><enclosure url="lib/media/pasted-image-20241216110120.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241216110120.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Cluster Editing Reduction]]></title><description><![CDATA[ 
 <br>Given the <a data-href="Cluster Editing" href="algorithms-ii-concepts/cluster-editing.html" class="internal-link" target="_self" rel="noopener nofollow">Cluster Editing</a> problem, we employ the following reduction rules:<br>
(1) Many shared neighbours<br>
<img alt="Pasted image 20241214132714.png" src="lib/media/pasted-image-20241214132714.png"><br>
(2) Many unique neighbours<br>
<img alt="Pasted image 20241214132805.png" src="lib/media/pasted-image-20241214132805.png"><br>
(3) Remove cliques<br>
Simple as that.<br><br>(1) Many shared neighbours<br>Show that  has a cluster edit of size at most    has a cluster edit of size at most . <br> If  is a cluster graph obtainable from  using at most  edits, then it is a cluster graph obtainable from  using at most  edits (with the additional edit being adding ). <br> If  is a cluster graph obtainable form  using at most  edits, then it must contain . Assume, for the sake of contradiction, that it does not. Then,  and  must belong to different clusters which implies that there exist  edge-disjoint paths between , and the  edges (see above), which would require more than  edits to handle. <br>(2) Many unique neighbours<br> If  is a cluster graph obtainable from  using at most  edits, then it is a cluster graph obtainable from  using at most  edits (with the additional edit being removing ). <br> If  is a cluster graph obtainable form  using at most  edits, then it must not contain . Assume, for the sake of contradiction, that it does. Then, it would take at least  edits to make  a cluster graph (see above). So, it must be true that  does not contain , which means that we would have to turn  into  before getting . <br>(3) Remove Cliques<br>Let  be the graph produced by removing all the cliques in , and let  be the set of edits needed for  to be a cluster graph . Then, applying  to  would produce  which doesn't take any more edits.<br><br>(1)  is a set of cluster edits that operate solely within connected components of <br>Consider the connected components  of , and the set of edits  employed on each. A cluster graph  can be obtained from  by applying . Therefore,  can be obtained using . Any  would not be a minimum cluster edit set.<br>(2) A fully-reduced yes instance  has at most  vertices and  edges<br>From (1), we saw that we can partition  into . Since we removed all cliques, then we know that each  has to involve at least one edit. <br>INCOMPLETE]]></description><link>algorithms-ii-algorithms/cluster-editing-reduction.html</link><guid isPermaLink="false">Algorithms II Algorithms/Cluster Editing Reduction.md</guid><pubDate>Fri, 20 Dec 2024 15:46:46 GMT</pubDate><enclosure url="lib/media/pasted-image-20241214132714.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241214132714.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Crown Reduction]]></title><description><![CDATA[ 
 ]]></description><link>algorithms-ii-algorithms/crown-reduction.html</link><guid isPermaLink="false">Algorithms II Algorithms/Crown Reduction.md</guid><pubDate>Sat, 14 Dec 2024 18:36:41 GMT</pubDate></item><item><title><![CDATA[Edmonds-Karp]]></title><description><![CDATA[ 
 <br>A greedy strategy for finding an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a> during the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a>. Run the BFS in  starting from  to find a shortest <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>, then proceed with the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> as usual. It runs in  time.]]></description><link>algorithms-ii-algorithms/edmonds-karp.html</link><guid isPermaLink="false">Algorithms II Algorithms/Edmonds-Karp.md</guid><pubDate>Sun, 08 Dec 2024 22:38:25 GMT</pubDate></item><item><title><![CDATA[Edmonds's Algorithm]]></title><description><![CDATA[<a class="tag" href="?query=tag:todo" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#todo</a> <a class="tag" href="?query=tag:todo" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#todo</a> 
 <br>To find a <a data-href="General Maximum-Cardinality Matching" href="algorithms-ii-algorithms/general-maximum-cardinality-matching.html" class="internal-link" target="_self" rel="noopener nofollow">General Maximum-Cardinality Matching</a>, <br>Like other <a data-href="Bipartite Maximum-Cardinality Matching" href="algorithms-ii-algorithms/bipartite-maximum-cardinality-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Bipartite Maximum-Cardinality Matching</a> algorithms, Edmonds's starts with either an empty or maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> (which can me computed in  time), then grows it by performing . <br><br><br> We start by computing an <a data-href="Alternating Forest" href="algorithms-ii-concepts/alternating-forest.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating Forest</a>  of , with all the unmatched vertices as its roots.<br>Claim: If  and  are both even vertices and there exists no edges  in the computed forest, then the <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> that exists in the forest, , is a maximum-cardinality <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>. <br>
<br><a href=".?query=tag:todo" class="tag" target="_blank" rel="noopener nofollow">#todo</a> the proof for this
<br>Using the above claim, we do the following: <br>
<br>Inspect all edges of 
<br>If we find no unmatched edges  (described above), then  is a maximum-cardinality <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>
<br>Otherwise: 

<br> and  belong to different trees. The path  is an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>.
<br> and  belong to the same tree.  and  share at least one vertex in common: the root of the tree. 

<br>We let the set of edges in either  OR  plus  be the Blossom of the tree, while the rest is the stem.
<br><img alt="Pasted image 20241215194658.png" src="lib/media/pasted-image-20241215194658.png">
<br>Next, we contract this blossom. Meaning, we remove all the vertices/edges in it and replace them with a new vertex . This results in a new <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  that may contain an edge between some vertex  and . 
<br>We can then recurse on the new graph that this contraction gives us such that we assume that  is a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of    is a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of . So, if we can't find any augmenting paths we know that we found a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>. 
<br>We can construct a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  from the <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>  of  by Expanding blossoms.




<br>Expanding blossoms<br><img alt="Pasted image 20241215205126.png" src="lib/media/pasted-image-20241215205126.png"><br>
(Norbert's work)<br>Consider the <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>  in  whose endpoints are  and . If  is not on the path, then we're chilling.<br>Otherwise, let  be an unmatched edge. Since  is an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>,  (the path from  to ) is an odd-length, alternating one. Let  be the "first" vertex in  that is "not on the direction of " in the path. Then, if we were to add the edges in  to the path between  and , we would have an even + odd = odd length path from  to .<br>
If  is unmatched, then this new path from  to  is easily an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>.<br>
If  is matched by , then it must have been that the edge  was in the <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> when the blossom was contracted, which means that  cannot have been , and that the subpath from  to  is an alternating path whose last edge is unmatched. Concatenating  and  and  therefore yields an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>.<br><br><br>The very detailed description above shows that if  is an augmenting path for , then expanding the blossom will always give us an augmenting path for  as well.<br>Assume, that  is not a maximum-cardinality matching and show that this implies that  is not a maximum cardinality matching either.<br>
 Consider the graph  (every edge that is only in either the stem of the matching), as shown below,  would be a matching as well, whose size is equivalent to that of <br>
<br>
<a href=".?query=tag:todo" class="tag" target="_blank" rel="noopener nofollow">#todo</a> 🔺 Incomplete proof


<br><br>
<br> time to construct the <a data-href="Alternating BFS" href="algorithms-ii-algorithms/alternating-bfs.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating BFS</a>
<br> time to inspect all the edges and check for unmatched 
<br> time to determine whether  are on the same tree.
<br> time to construct blossom  and resulting graph and <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>.
<br>Since we recurse on the graphs constructed by blossoms, the running time of each blossom recurrence is 
<br><img alt="Pasted image 20241215195936.png" src="lib/media/pasted-image-20241215195936.png">
<br>because we remove at least two vertices per blossom, and each blossom has at least  vertices. The solution to this recurrence is 
]]></description><link>algorithms-ii-algorithms/edmonds&apos;s-algorithm.html</link><guid isPermaLink="false">Algorithms II Algorithms/Edmonds&apos;s Algorithm.md</guid><pubDate>Mon, 16 Dec 2024 04:33:17 GMT</pubDate><enclosure url="lib/media/pasted-image-20241215194658.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241215194658.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Ford-Fulkerson]]></title><description><![CDATA[ 
 <br>For each iteration, look for the <a data-href="Residual Network" href="algorithms-ii-concepts/residual-network.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Network</a>  of . If there exists any -path  in this network:<br>
<br>Find the minimum <a data-href="Residual Capacity" href="algorithms-ii-concepts/residual-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Capacity</a> in , call it 
<br>If  is that of , then  gets set to  and  gets set to the capacity, otherwise<br>
<img alt="center" src="lib/media/pasted-image-20241208172806.png" style="width: 300px; max-width: 100%;"><br>
Note that the Ford-Fulkerson may not terminate for some inputs.
<br>Correctness<br>Once we prove that every iteration maintains a feasible flow, and since this terminates once there do not exist any <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>  in the network: <br>
<br>The flow across the network,  is bottle-necked by the some edges.
<br>Since we run Ford-Fulkerson until these bottle-neck edges are filled up, and since these edges are going to have the minimum capacities, then the -cut involving those edges is a  minimum -cut whose capacity = . 
<br>By the <a data-href="Max-flow Min-cut" href="algorithms-ii-concepts/max-flow-min-cut.html" class="internal-link" target="_self" rel="noopener nofollow">Max-flow Min-cut</a> theorem, this implies that the current flow is a maximum flow.
]]></description><link>algorithms-ii-algorithms/ford-fulkerson.html</link><guid isPermaLink="false">Algorithms II Algorithms/Ford-Fulkerson.md</guid><pubDate>Sun, 08 Dec 2024 22:23:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20241208172806.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241208172806.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[General Maximum-Cardinality Matching]]></title><description><![CDATA[ 
 <br>While we can still use the algorithms described to compute a <a data-href="Bipartite Maximum-Cardinality Matching" href="algorithms-ii-algorithms/bipartite-maximum-cardinality-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Bipartite Maximum-Cardinality Matching</a>, it would take us longer to find an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>. <br>The main two problems are: <br>
<br>We relied on the fact that every <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a> will have an odd length, start with an unmatched vertex , and end with an unmatched vertex , which allowed us to find such a path in  time.
<br>More importantly, the existence of odd-length cycles makes the behaviour of the alternating DFS/BFS unpredictable. The algorithm would have to traverse such cycles "in the right direction" to be able to find an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>.
<br>We will be using <a data-href="Edmonds's Algorithm" href="algorithms-ii-algorithms/edmonds's-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Edmonds's Algorithm</a> to find our <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>.]]></description><link>algorithms-ii-algorithms/general-maximum-cardinality-matching.html</link><guid isPermaLink="false">Algorithms II Algorithms/General Maximum-Cardinality Matching.md</guid><pubDate>Mon, 16 Dec 2024 04:33:17 GMT</pubDate></item><item><title><![CDATA[MAX-SAT Randomized Rounding]]></title><description><![CDATA[ 
 <br>We employ a max of naive guessing and LP rounding:<br>
<img alt="Pasted image 20241216132913.png" src="lib/media/pasted-image-20241216132913.png">]]></description><link>algorithms-ii-algorithms/max-sat-randomized-rounding.html</link><guid isPermaLink="false">Algorithms II Algorithms/MAX-SAT Randomized Rounding.md</guid><pubDate>Mon, 16 Dec 2024 17:29:17 GMT</pubDate><enclosure url="lib/media/pasted-image-20241216132913.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241216132913.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Maximal matching]]></title><description><![CDATA[ 
 <br>Inspect every edge in the graph. <br>If neither of its endpoints are unmatched, add it to the <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>.<br>This can be done in  time as we can mark every vertex as unmatched in  time then inspect every edge in . ]]></description><link>algorithms-ii-algorithms/maximal-matching.html</link><guid isPermaLink="false">Algorithms II Algorithms/Maximal matching.md</guid><pubDate>Sun, 15 Dec 2024 18:06:04 GMT</pubDate></item><item><title><![CDATA[Maximum Independent Set Branching]]></title><description><![CDATA[ 
 <br>Very similar to the <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> <a data-href="Branching" href="algorithms-ii-concepts/branching.html" class="internal-link" target="_self" rel="noopener nofollow">Branching</a> algorithm, our "elementary choice" is whether a vertex should be the independent set or not. <br>At each iteration, we compute the independent set of  and , call them , respectively, and we output the larger of  and .<br>The biggest difference between this and the <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> <a data-href="Branching" href="algorithms-ii-concepts/branching.html" class="internal-link" target="_self" rel="noopener nofollow">Branching</a> is that if  is an isolated graph, then we simply return all the vertices in it.<br>The running time analysis of this is exactly the same as the <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> running time analysis. ]]></description><link>algorithms-ii-algorithms/maximum-independent-set-branching.html</link><guid isPermaLink="false">Algorithms II Algorithms/Maximum Independent Set Branching.md</guid><pubDate>Mon, 16 Dec 2024 17:06:36 GMT</pubDate></item><item><title><![CDATA[Preflow-Push]]></title><description><![CDATA[<a class="tag" href="?query=tag:todo" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#todo</a> 
 <br>Assume that  initially "holds" an amount of flow (litres of water) equivalent to the sum of the capacities of its out edges. <br>We can make this water flow to the rest of the graph by raising 's altitude, so the water "trickles" down.<br>While lifting  and its other vertices, we may have excess at some of the other nodes. We can raise these nodes above  so that the excess flows back into . <br>We use the following algorithm<br>
 Initialize the height of  to be the number of vertices in the network.<br>
 For every edge  in the <a data-href="Residual Network" href="algorithms-ii-concepts/residual-network.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Network</a> , the difference between the height of  and the height of  never exceeds .<br>
 If there exists any excess flow at a vertex , and there is a vertex  whose height is , push the smaller of that excess and the <a data-href="Residual Capacity" href="algorithms-ii-concepts/residual-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Capacity</a> of  down to .<br>
 If  does not have a  whose height is less than , then we relabel the height of  to be  higher than its shortest neighbour. <br>As long as  is not a feasible -flow, either PUSH or RELABEL are applicable<br>If  is not feasible, then there exist an excess flow at one of the vertices.<br>There is no path between  and  in the <a data-href="Residual Network" href="algorithms-ii-concepts/residual-network.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Network</a> at termination<br>If there is, then there exists a <a data-href="Residual Capacity" href="algorithms-ii-concepts/residual-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Capacity</a> at every edge on that path. The existence of a <a data-href="Residual Capacity" href="algorithms-ii-concepts/residual-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Capacity</a> on every edge means that every vertex on that path is at most one unit of height higher than the one before it. This gives us <br><br>Where  is the number of vertices on the path, which would be at most . This gives us a contradiction and, therefore, proves our point.<br>The flow after termination is feasible<br>Since there is no path between  and  in the <a data-href="Residual Network" href="algorithms-ii-concepts/residual-network.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Network</a> at termination, by the <a data-href="Max-flow Min-cut" href="algorithms-ii-concepts/max-flow-min-cut.html" class="internal-link" target="_self" rel="noopener nofollow">Max-flow Min-cut</a> theorem,  equals the capacity of some -cut, which makes it a maximum flow. <br>Running time<br>First, we should prove that there always exists a path between a vertex  and  in the <a data-href="Residual Network" href="algorithms-ii-concepts/residual-network.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Network</a> of .<br>
Let  be the set of vertices (other than ) reachable from  in , and assume that  is not among those vertices. Then the sum of the excess flow at all vertices in  is<br>
<img alt="Pasted image 20241214190430.png" src="lib/media/pasted-image-20241214190430.png"><br>
Which implies that there exists an edge , and therefore, that  must also be reachable form .<br>
<br><a href=".?query=tag:todo" class="tag" target="_blank" rel="noopener nofollow">#todo</a> how is this specific to ?
<br>s AND t ARE NEVER RELABELLED<br>Every vertex  has a height of at most <br>Since we showed that there always exists a path between  and  in . We know that the height of  will be at most the height of .<br>
Since the height of  is initialized to  and there are at most  vertices on the path, each of them at most one unit of height higher than the one before. If we relabel , we may end up increasing the height of all the vertices on this path by  as well, thus yielding a total height difference of . <br>We perform at most  relabel operations<br>Since  and  are never relabelled and the height of  is at most , with each relabel increasing the height of  by at least , we can conclude that  is relabelled at most  times. Since there are  vertices that could get relabelled, the total count is . <br>We perform at most  saturating push operations<br>
 Instead, let's show that there are at most  saturating push operations per edge. <br>Considering the saturating push operation from  to . Here the height of  is , one greater than the height of way. Since this is a saturating push, we would have to relabel , push from it to , then relabel  so that it can push to . Therefore, the height of  increases by  between each two saturating push operations on the edge . Since we showed that the height of  is at most . This yields at most  saturating push operations per edge. Since the same argument applies to , this adds up to a total of  saturating push operations on , and therefore,  saturating push operations for all the edges. <br>At most  non-saturating push operations<br>Define the potential of an iteration as the sum of the heights of the excess vertices.<br> A relabel operation increases the potential by at most  for each vertex.<br>
 A saturating push operation from  to  may result in  becoming an excess vertex when it wasn't before, and therefore, may end up adding its height  to the potential. Since that height is , the operation increases the potential at most by that amount.<br>The total increase by relabel and saturating push operations is therefore . <br> A non-saturating push from  to  will set  and possibly create excess at  when there wasn't any before. Therefore, the total change in height would be the difference between their heights, which needs to always be . Since the height of  is actually less than the height of , this change is actually a decrease by 1.<br>Therefore, a non-saturating push can decrease at most what has been increased, . ]]></description><link>algorithms-ii-algorithms/preflow-push.html</link><guid isPermaLink="false">Algorithms II Algorithms/Preflow-Push.md</guid><pubDate>Sun, 15 Dec 2024 00:26:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20241214190430.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241214190430.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Set Cover Dual Fitting]]></title><description><![CDATA[<a class="tag" href="?query=tag:todo" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#todo</a> 
 <br>Consider the LP relaxation of the ILP formulation of the <a data-href="Set Cover" href="algorithms-ii-concepts/set-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Set Cover</a> problem<br>
<img alt="Pasted image 20241209210504.png" src="lib/media/pasted-image-20241209210504.png"><br>
And its <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a><br>
<img alt="Pasted image 20241209210522.png" src="lib/media/pasted-image-20241209210522.png"><br>
We can think of this <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> as "packing" the values  for each element  into  without exceeding its weight capacity . <br>If we apply the <a data-href="Set Cover Greedy" href="algorithms-ii-algorithms/set-cover-greedy.html" class="internal-link" target="_self" rel="noopener nofollow">Set Cover Greedy</a> algorithm, it's possible that the sum<br>
exceeds the total weight of , , because there may be some elements in  that were assigned a weight and added to  by some other set. This makes the solution  infeasible. <br>This makes sense because if it was feasible, then the primal corresponding to it would be optimal, which would give us a solution to the <a data-href="Set Cover" href="algorithms-ii-concepts/set-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Set Cover</a> problem that is computable in polynomial time.<br>Now, consider the solution<br>
Claim: this is a feasible solution of the <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a><br>Proof: <br>Order the elements in some set , whose size is , by the order in which they are covered: <br>Consider when  is added. At this point, at least  are uncovered, and so, the cost of adding  to  would be<br>
Since each iteration picks the set that would cover  with minimum cost, we observe that<br>
And since <br>
Which means that such a solution is feasible, and therefore, by <a data-href="Weak Duality" href="algorithms-ii-concepts/weak-duality.html" class="internal-link" target="_self" rel="noopener nofollow">Weak Duality</a>, that a feasible solution to the primal will be an -<a data-href="Approximation" href="algorithms-ii-concepts/approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Approximation</a> to an optimal solution. <br>
<br>Double check the above conclusion <a href=".?query=tag:todo" class="tag" target="_blank" rel="noopener nofollow">#todo</a>
]]></description><link>algorithms-ii-algorithms/set-cover-dual-fitting.html</link><guid isPermaLink="false">Algorithms II Algorithms/Set Cover Dual Fitting.md</guid><pubDate>Fri, 13 Dec 2024 23:56:55 GMT</pubDate><enclosure url="lib/media/pasted-image-20241209210504.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241209210504.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Set Cover Greedy]]></title><description><![CDATA[<a class="tag" href="?query=tag:todo" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#todo</a> 
 <br>Two greedy strategies to solving the <a data-href="Set Cover" href="algorithms-ii-concepts/set-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Set Cover</a> problem come to mind: <br>
<br>Pick the cheapest sets first 
<br>Pick the sets that cover the most elements first
<br>However, each may be ineffective on its own, depending on the given input. <br>So, we try to combine them. We look at the weight per element covered, which means minimizing<br>
However, this can also be ineffective as we should be paying more attention to the elements not yet covered at each choice we make. <br>So, we consider the cost per newly covered element at each iteration<br>
<img alt="Pasted image 20241209162211.png" src="lib/media/pasted-image-20241209162211.png"><br><br>Claim 1: The algorithm computes an  <a data-href="Approximation" href="algorithms-ii-concepts/approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Approximation</a> of an optimal <a data-href="Set Cover" href="algorithms-ii-concepts/set-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Set Cover</a>. <br>Proof 1:<br>Let<br>
Be the price that we assigned to each element in  for a given iteration.<br>
_<br>
Claim 1.1: $$p{e{i}} \leq \frac{OPT}{n - i + 1}$$Proof 1.1:<br>
<br>
Claim 1.1.1:<br>Order the elements (not sets!) in  by the order in which they were added to yield the sequence . Consider the  element being added to  and the set that contains it, . Prove that<br>
<br>Proof 1.1.1: <br>Assign each element  not in  to exactly one set  also not in . Let the collection of such sets be called , and let<br>
Observe that<br>
<br>
<br><a href=".?query=tag:todo" class="tag" target="_blank" rel="noopener nofollow">#todo</a> How do we know that it's &lt;= OPT?
<br>In which there must exist an element that satisfies<br>
Because  is the average.<br>
_<br>Since each set must contain at least one element, and since the  elements are not yet in ,<br>
__<br>Finally, note that , and <br><img alt="Pasted image 20241209172948.png" src="lib/media/pasted-image-20241209172948.png"><br>
▨]]></description><link>algorithms-ii-algorithms/set-cover-greedy.html</link><guid isPermaLink="false">Algorithms II Algorithms/Set Cover Greedy.md</guid><pubDate>Mon, 09 Dec 2024 21:44:14 GMT</pubDate><enclosure url="lib/media/pasted-image-20241209162211.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241209162211.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Simplex]]></title><description><![CDATA[<a class="tag" href="?query=tag:todo" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#todo</a> 
 <br>We take an LP in <a data-href="Standard form" href="algorithms-ii-concepts/standard-form.html" class="internal-link" target="_self" rel="noopener nofollow">Standard form</a>. <br>1. Does it have a feasible solution?<br>
<br><a href=".?query=tag:todo" class="tag" target="_blank" rel="noopener nofollow">#todo</a> finish this, I'm too bored to do that
]]></description><link>algorithms-ii-algorithms/simplex.html</link><guid isPermaLink="false">Algorithms II Algorithms/Simplex.md</guid><pubDate>Sat, 14 Dec 2024 22:07:58 GMT</pubDate></item><item><title><![CDATA[Steiner Tree DP]]></title><description><![CDATA[<a class="tag" href="?query=tag:todo" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#todo</a> <a class="tag" href="?query=tag:p" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#p</a> 
 <br>First, we want to transform the input such that every vertex that needs to be in the tree is guaranteed to be a leaf (degree ). <br>We do this by creating a twin for each vertex , and connecting each pair of twins by and edge of weight . This transformed graph is called . <br>Claim:  is a solution to the <a data-href="Steiner Tree" href="algorithms-ii-concepts/steiner-tree.html" class="internal-link" target="_self" rel="noopener nofollow">Steiner Tree</a> problem for    is a solution of the <a data-href="Steiner Tree" href="algorithms-ii-concepts/steiner-tree.html" class="internal-link" target="_self" rel="noopener nofollow">Steiner Tree</a> problem for  where . <br>Proof:<br>If  is a <a data-href="Steiner Tree" href="algorithms-ii-concepts/steiner-tree.html" class="internal-link" target="_self" rel="noopener nofollow">Steiner Tree</a> for , then it contains all the vertices in . So,  would be a <a data-href="Steiner Tree" href="algorithms-ii-concepts/steiner-tree.html" class="internal-link" target="_self" rel="noopener nofollow">Steiner Tree</a>, too, and it contains all the vertices in . If we were to remove all vertices in  from  to get , then  would be a tree that contains all vertices in , which would make it a <a data-href="Steiner Tree" href="algorithms-ii-concepts/steiner-tree.html" class="internal-link" target="_self" rel="noopener nofollow">Steiner Tree</a> of , still.<br> because all the edges between twins are of weight . <br><br>A Steiner tree for  may NOT be composed of Steiner trees of its subsets<br>We shall instead consider all subsets  of  paired with a vertex . <br><img alt="Pasted image 20241216010451.png" src="lib/media/pasted-image-20241216010451.png"><br><br><br>
<br><a href=".?query=tag:todo" class="tag" target="_blank" rel="noopener nofollow">#todo</a> <a href=".?query=tag:p" class="tag" target="_blank" rel="noopener nofollow">#p</a> ⏫ 
<br><br><br><img alt="Pasted image 20241216010616.png" src="lib/media/pasted-image-20241216010616.png">]]></description><link>algorithms-ii-algorithms/steiner-tree-dp.html</link><guid isPermaLink="false">Algorithms II Algorithms/Steiner Tree DP.md</guid><pubDate>Mon, 16 Dec 2024 05:32:06 GMT</pubDate><enclosure url="lib/media/pasted-image-20241216010451.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241216010451.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Steiner Tree Simple]]></title><description><![CDATA[ 
 <br>We can compute the <a data-href="Steiner Tree" href="algorithms-ii-concepts/steiner-tree.html" class="internal-link" target="_self" rel="noopener nofollow">Steiner Tree</a> of a subset  through the following steps: <br>
<br>Calculate all possible subsets of 
<br>For all subsets , calculating the MST of  (in polynomial time). 
<br>Search for the minimum of the minimum spanning trees and report.
<br>Since there are  subsets, this algorithm takes  time.]]></description><link>algorithms-ii-algorithms/steiner-tree-simple.html</link><guid isPermaLink="false">Algorithms II Algorithms/Steiner Tree Simple.md</guid><pubDate>Mon, 16 Dec 2024 05:32:06 GMT</pubDate></item><item><title><![CDATA[The Hungarian Algorithm]]></title><description><![CDATA[ 
 <br>The <a data-href="Bipartite Minimum-Weight Perfect Matching LP" href="algorithms-ii-algorithms/bipartite-minimum-weight-perfect-matching-lp.html" class="internal-link" target="_self" rel="noopener nofollow">Bipartite Minimum-Weight Perfect Matching LP</a> formulation gave us the following property<br><img alt="Pasted image 20241215152300.png" src="lib/media/pasted-image-20241215152300.png"><br>Now, our goal will be to apply the <a data-href="Primal-Dual Schema" href="algorithms-ii-concepts/primal-dual-schema.html" class="internal-link" target="_self" rel="noopener nofollow">Primal-Dual Schema</a> to find an optimal solution (minimum-weight <a data-href="Perfect matching" href="algorithms-ii-concepts/perfect-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Perfect matching</a>) to the primal.<br>
<br>We initialize all  and . This is a feasible solution as it satisfies . 
<br>Look for a tight <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>  for  and perform . 

<br>If there is no tight <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>, then adjust the potential function  such that we can find one.


<br>Since a <a data-href="Perfect matching" href="algorithms-ii-concepts/perfect-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Perfect matching</a> of a <a data-href="Complete Bipartite Graph" href="algorithms-ii-concepts/complete-bipartite-graph.html" class="internal-link" target="_self" rel="noopener nofollow">Complete Bipartite Graph</a> will have exactly  edges in it, we should be able to find a <a data-href="Perfect matching" href="algorithms-ii-concepts/perfect-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Perfect matching</a> within  iterations of the steps above.<br>Tight: one that satisfies  for every edge  on that path. <br>Look: Using the <a data-href="Alternating BFS" href="algorithms-ii-algorithms/alternating-bfs.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating BFS</a> routine, we will construct a an <a data-href="Alternating Forest" href="algorithms-ii-concepts/alternating-forest.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating Forest</a> based on the subgraph  that consists only of tight edges. We can then look for an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a> in  time.<br>Adjust:<br><img alt="Pasted image 20241215162433.png" src="lib/media/pasted-image-20241215162433.png"><br>
We would have to perform the above at most  times to make sure that all the vertices in  are in the tight <a data-href="Alternating Forest" href="algorithms-ii-concepts/alternating-forest.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating Forest</a>.<br>
 As long as  is not a <a data-href="Perfect matching" href="algorithms-ii-concepts/perfect-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Perfect matching</a>, as justified in the proof of the <a data-href="Alternating BFS" href="algorithms-ii-algorithms/alternating-bfs.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating BFS</a> routine, then there will exist an unmatched vertex in . <br>Therefore, during each phase, we will have at most  adjustment iterations.<br>Proofs:<br>1.  is feasible<br>Observe that<br>
<img alt="Pasted image 20241215165038.png" src="lib/media/pasted-image-20241215165038.png"><br>
Which implies that<br>
<img alt="Pasted image 20241215165018.png" src="lib/media/pasted-image-20241215165018.png"><br>
Therefore, unless  and , it will trivially be true that<br>
If  and  then<br>
<img alt="Pasted image 20241215165249.png" src="lib/media/pasted-image-20241215165249.png"><br>
So, we're good.<br>2. Before augmentation, <br> Assume that . Then there exists a tight alternating path between  and an unmatched vertex  whose length is even. Since , it must be tight, and therefore, there exists a tight alternating path between  and , making . <br> Assume that . Then there exists an odd-length tight alternating path between  and an unmatched vertex . Since , it is a tight edge, adding it to this path gives a tight alternating path between  and , which makes . <br>3. Adjustment always increase the number of vertices in the <a data-href="Alternating Forest" href="algorithms-ii-concepts/alternating-forest.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating Forest</a><br> If consider the vertex  that is in  but not in  but which has the shortest alternating path between itself and a root of the original forest. Then its predecessor  would be in the new <a data-href="Alternating Forest" href="algorithms-ii-concepts/alternating-forest.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating Forest</a>, meaning that there exists an alternating tight path between  and a root of the new forest. Since  is the old forest, it is tight and  would be an  pairing, which means that their potential function values don't change, which would make them a part of the new <a data-href="Alternating Forest" href="algorithms-ii-concepts/alternating-forest.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating Forest</a> as well because there would exist a tight alternating path between a root and . This is a contradiction, which means that every vertex in  will exist in . <br>Now, consider  such that  and . We already know that this edge is tight. Since , it must be true that there exists an even alternating tight path between  and some other vertex . Since this path has to end in an edge in , and we showed that  is in  , that that matched edge it ends in must be related to . So, there exists a tight alternating path between  and  which means that , and therefore, that . <br>Running time<br>We said earlier that we need  phases and each phase is  iterations, so a total of  iterations is ran. Running augment/adjust on each iteration takes  time because at the very least, we have to inspect every edge in the graph, and since the graph is complete, we have  edges.<br>Therefore, the overall running time is . ]]></description><link>algorithms-ii-algorithms/the-hungarian-algorithm.html</link><guid isPermaLink="false">Algorithms II Algorithms/The Hungarian Algorithm.md</guid><pubDate>Sun, 15 Dec 2024 22:11:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20241215152300.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241215152300.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Travelling Salesman Dynamic Programming]]></title><description><![CDATA[ 
 <br>We can use <a data-href="Dynamic Programming" href="algorithms-ii-concepts/dynamic-programming.html" class="internal-link" target="_self" rel="noopener nofollow">Dynamic Programming</a> to obtain nice solution to the TSP.<br><br><br>Let  be a subset of . <br>We call a path that visit all and only the vertices in  a -path.<br>Let  be some subset of vertices that starts at , and consider the hamiltonian cycle that starts and ends at . We call the path from  to the vertex right before it, , a v-path from  to . <br>Now, let  be the length of the shortest v-path between  snd  in the subset . <br>Then, an optimal TSP has length:<br>
So, we consider each possible .<br><br><br>We consider every possible subset  and vertex  in our computation of<br>
We fill a table whose entries are dependent on the size of , and the vertex . <br><img alt="Pasted image 20241216001433.png" src="lib/media/pasted-image-20241216001433.png"><br><br>We then recurse on the entries of the table using the following definition: <br>Base case: . <br>In this case,  if  and  otherwise.<br>Inductive step: <br>In this case, <br>]]></description><link>algorithms-ii-algorithms/travelling-salesman-dynamic-programming.html</link><guid isPermaLink="false">Algorithms II Algorithms/Travelling Salesman Dynamic Programming.md</guid><pubDate>Mon, 16 Dec 2024 04:33:17 GMT</pubDate><enclosure url="lib/media/pasted-image-20241216001433.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241216001433.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Vertex Cover 2-Approximation]]></title><description><![CDATA[ 
 <br>In a maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  of , every vertex in  covers at most one vertex in . Therefore, we need at least  vertices to be in a <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of . <br>A simple -<a data-href="Approximation" href="algorithms-ii-concepts/approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Approximation</a> simply adds the two vertices of every edge in  to the <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a>, thus yielding a <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of size . ]]></description><link>algorithms-ii-algorithms/vertex-cover-2-approximation.html</link><guid isPermaLink="false">Algorithms II Algorithms/Vertex Cover 2-Approximation.md</guid><pubDate>Sun, 15 Dec 2024 00:26:50 GMT</pubDate></item><item><title><![CDATA[Vertex Cover Branching]]></title><description><![CDATA[ 
 <br>Since the <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of some graph  must include a vertex  or all its neighbours, one <a data-href="Branching" href="algorithms-ii-concepts/branching.html" class="internal-link" target="_self" rel="noopener nofollow">Branching</a> algorithm to compute the <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of  recursively computes the <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of  and  then returns the smaller of  and .<br>Proof: If there existed a <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a>  of  that was smaller than the one produced by the algorithm above, then  which is a contradiction because  is the smallest <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of .<br>Running time: We make two recursive calls on  and , each with input size at most , so our <a data-href="Branching vector" href="algorithms-ii-concepts/branching-vector.html" class="internal-link" target="_self" rel="noopener nofollow">Branching vector</a> is  and therefore  is the root of the polynomial . Therefore, we make  recursive calls. Since each call would take  time, the overall running time is  time.<br>]]></description><link>algorithms-ii-algorithms/vertex-cover-branching.html</link><guid isPermaLink="false">Algorithms II Algorithms/Vertex Cover Branching.md</guid><pubDate>Mon, 16 Dec 2024 14:30:00 GMT</pubDate></item><item><title><![CDATA[Vertex Cover Branching Better]]></title><description><![CDATA[ 
 <br> If  then the difference between  and  is not just , making our <a data-href="Branching vector" href="algorithms-ii-concepts/branching-vector.html" class="internal-link" target="_self" rel="noopener nofollow">Branching vector</a> of  overly pessimistic.<br> If  then  is the only vertex in its neighbourhood, so it has no edges, and therefore, any <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of  is also a <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of . <br> The same logic applies to the <a data-href="Maximum Independent Set Branching" href="algorithms-ii-algorithms/maximum-independent-set-branching.html" class="internal-link" target="_self" rel="noopener nofollow">Maximum Independent Set Branching</a>. An independent set of  will be exactly and independent of  plus .<br>So, a s a better <a data-href="Branching" href="algorithms-ii-concepts/branching.html" class="internal-link" target="_self" rel="noopener nofollow">Branching</a> rule, we only branch when , where this gives us the <a data-href="Branching vector" href="algorithms-ii-concepts/branching-vector.html" class="internal-link" target="_self" rel="noopener nofollow">Branching vector</a> , and a <a data-href="Branching number" href="algorithms-ii-concepts/branching-number.html" class="internal-link" target="_self" rel="noopener nofollow">Branching number</a> of . <br> We can do even better by considering the fact that any degree  vertex will have to be the one covering its edge (or its neighbour would). <br> Therefore, we don't even branch if  is a degree- vertex, meaning that the size of the neighbourhood of  would have to be , and therefore, that the <a data-href="Branching vector" href="algorithms-ii-concepts/branching-vector.html" class="internal-link" target="_self" rel="noopener nofollow">Branching vector</a> becomes  with a <a data-href="Branching number" href="algorithms-ii-concepts/branching-number.html" class="internal-link" target="_self" rel="noopener nofollow">Branching number</a> of . ]]></description><link>algorithms-ii-algorithms/vertex-cover-branching-better.html</link><guid isPermaLink="false">Algorithms II Algorithms/Vertex Cover Branching Better.md</guid><pubDate>Mon, 16 Dec 2024 17:06:36 GMT</pubDate></item><item><title><![CDATA[Vertex Cover Crown Reduction]]></title><description><![CDATA[<a class="tag" href="?query=tag:todo" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#todo</a> 
 <br>In <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> reduction, we obtained a kernel of size , which is not good because the running time of <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> is . <br>We want a kernel of size  instead, using <a data-href="Crown" href="algorithms-ii-concepts/crown.html" class="internal-link" target="_self" rel="noopener nofollow">Crown</a> reduction.<br>Our only rule is:<br>
Compute a maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> <br>
Then compute a <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> between all the vertices in  and those not in <br>
If that <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> matches all the vertices not in , then  is our kernel.<br>
Otherwise,  is our kernel. <br>Is this a sound reduction?<br>If we find the <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>, the  is the kernel. Since  remains unchanged, we don't have to worry about that part. <br>If we don't find the <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> then<br>
If , then  has no <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of size , and we have a no-instance.<br>
Else if , then  might be a yes-instance. We need to prove that if  is a yes-instance   is a yes-instance.<br>Let  and <br>If  is a yes-instance then there exists a <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a>  of  whose size is at most .<br>
But, more importantly, the set  is a <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of  because every vertex in  is matched by such a <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>, and as we showed in the description of <a data-href="Crown" href="algorithms-ii-concepts/crown.html" class="internal-link" target="_self" rel="noopener nofollow">Crown</a>. <br>If  is a yes-instance with <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> , then  is a yes-instance if . <br>
<br><a href=".?query=tag:todo" class="tag" target="_blank" rel="noopener nofollow">#todo</a> incomplete 🔺 
<br>If  is a yes-instance then it has  vertices]]></description><link>algorithms-ii-algorithms/vertex-cover-crown-reduction.html</link><guid isPermaLink="false">Algorithms II Algorithms/Vertex Cover Crown Reduction.md</guid><pubDate>Sun, 15 Dec 2024 00:26:50 GMT</pubDate></item><item><title><![CDATA[Vertex Cover Exponential]]></title><description><![CDATA[ 
 <br><br>A naive <a data-href="Exponential" href="algorithms-ii-concepts/exponential.html" class="internal-link" target="_self" rel="noopener nofollow">Exponential</a> algorithm is to find all possible subsets of the set of vertices  with size . There are  such subsets, and for each, we check if all the edges are covered which takes  time. Therefore, the overall running time is<br>
<br><br>For each edge , return the smaller of  and . <br>Each recursive call is implemented in  time, and we have one less vertex to work with in the following call, this gives us<br>
<br><br>The above algorithm can be redundant if the optimal <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> contains both vertices of some edge (as both branches would potentially compute the same <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a>). <br>A better approach is to return the smaller of  and  where  is the <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of , etc. for each vertex  of maximum degree. <br>Since  is of maximum degree, , which gives us the recurrence<br>
Whose solution happens to be . ]]></description><link>algorithms-ii-algorithms/vertex-cover-exponential.html</link><guid isPermaLink="false">Algorithms II Algorithms/Vertex Cover Exponential.md</guid><pubDate>Mon, 09 Dec 2024 23:49:06 GMT</pubDate></item><item><title><![CDATA[Vertex Cover Reduction]]></title><description><![CDATA[ 
 <br>We employ three reduction rules<br>
Singleton: Remove -degree vertices.<br>
Degree-1: If  is a degree- vertex, remove  and all edges incident to . Decrease  by .<br>
High-degree: If the degree of  is , remove  and all edges incident to it. Decrease  by . <br>If  is fully-reduced and  is a yes-instance then  and . <br>Since  is fully-reduced, we know that we have at least  edges per vertex and no more than . Therefore, for each vertex in the  vertices that cover , we have at most  edges, adding up to a total of  edges. <br>if  then, to prove that we need to show that . <br>Consider every edge in , there are  of them. If an edge  has that  then  must be true. Therefore, there is at most one vertex in  per edge, which means that there are at most  vertices in , which therefore proves our claim. ]]></description><link>algorithms-ii-algorithms/vertex-cover-reduction.html</link><guid isPermaLink="false">Algorithms II Algorithms/Vertex Cover Reduction.md</guid><pubDate>Sat, 14 Dec 2024 01:06:29 GMT</pubDate></item><item><title><![CDATA[Assignment 1 Summative]]></title><description><![CDATA[ 
 <br><br><br>Since the&nbsp;dominating set problem is a subset selection problem where we choose a subset  of vertices to be the dominating set, we shall represent  as the set of variables&nbsp;&nbsp;such that&nbsp;&nbsp;if&nbsp;&nbsp;and&nbsp;&nbsp;if&nbsp;. The weight of the dominating set &nbsp;is then<br><br>And is the objective function to be minimized. The constraints of the ILP need to enforce that&nbsp; is a dominating set. To be able to define the dominating set of this graph, we need to introduce the concept of the closed neighbourhood of a vertex  in a graph . The&nbsp;closed neighbourhood&nbsp;of a vertex&nbsp;&nbsp;in a graph&nbsp;, denoted ,&nbsp;is the subgraph of  composed of the vertices adjacent to&nbsp;, all edges connecting vertices adjacent to&nbsp;, and  itself. We denote the set of vertices in the closed neighbourhood of vertex  in a graph  as . Our goal is to choose the vertices of our dominating set such that for any vertex , there exists at least one vertex in its closed neighbourhood that is in . This can be translated into the following constraint <br><br>Combining the above constraint with the defined objective function yields the following ILP<br><br><br>We need to prove that a solution to this ILP, , is feasible if and only if it corresponds to a dominating set of .<br>Claim: If  is feasible, then it corresponds to a dominating set of .<br>Proof: <br>Assume  is a feasible solution to the ILP, and let it correspond to a set . Suppose, for the sake of contradiction, that  is not a dominating set of . This implies that there exists a vertex  such that  and none of the vertices in the closed neighbourhood  of  belong to . In terms of the ILP, this means that for this vertex , none of the variables corresponding to vertices in  have an objective value of .<br>However, this contradicts the feasibility of  because one of the constraints of the ILP requires that for every vertex , the sum of the variables corresponding to the vertices in  must be at least . Therefore, our assumption that  is not a dominating set leads to a contradiction, and thus  must be a dominating set.<br>**Claim: If  corresponds to a dominating set of , then it is feasible. <br>Proof:<br>Assume that  is a solution to the ILP and that it corresponds to a a dominating set of , . Suppose, for the sake of contradiction, that  is not a feasible solution, then this implies that for some vertex ,  which means that there exists a vertex  in  that is neither in the dominating set  nor is it adjacent to a vertex in . <br>However, this contradicts our assumption that  is a dominating set as it implies that not every vertex  has a neighbour in . Therefore, our assumption that  is not a feasible solution leads to a contradiction, and thus  must be a feasible solution. <br><br>We need to prove that  is an optimal solution of the ILP if and only if it corresponds to a dominating set of , , of minimum weight. <br>Claim: if  is an optimal solution of the ILP then it corresponds to a dominating set  of minimum weight<br>Proof:<br>Assume the  is an optimal solution of the ILP and that it corresponds to a dominating set . Suppose, for the sake of contradiction, that there exists a dominating set of vertices  such that , implying that  is not a minimum weight dominating set. If we let  be the solution of the ILP corresponding to , then this implies that . <br>However, this contradicts our assumption that  is an optimal solution. Therefore, our assumption that  is not a minimum weight dominating set leads to a contradiction, and thus  must be a minimum weight dominating set. <br>Claim: if  is dominating set of minimum weight then its corresponding ILP solution  is an optimal solution<br>Proof:<br>Assume that f  is dominating set of minimum weight and that it has a corresponding ILP solution . Suppose, for the sake of contradiction that  was not an optimal solution, implying that there exists a solution  such that . Let the dominating set represented by  be . Since  and , it must be true that . <br>However, this contradicts our assumption that  is a dominating set. Therefore, our assumption that  is not an optimal solution leads to a contradiction, and thus  must be an optimal solution of the ILP. <br><br><br>Claim: The solution of the ILP for this set  has objective function value .<br>Proof: Since all the intervals in  all overlap ( for all ), we can only select at most one interval for our solution. In the ILP, this means setting  for some  and  for all  such that the constraints<br><br>Are satisfied with maximal value. Therefore, the sum , which represents the objective function, must equal 1.<br>Claim: The LP relaxation of the ILP has a fractional solution with objective function value <br>An LP relaxation of this ILP would look identical to the ILP except that the constraint  is relaxed to be .<br>Proof: While it is also true that all the intervals in  overlap in this case, the LP relaxation gives us the option of setting   for all  such that the constraints <br><br>are satisfied with maximal value. Therefore, the sum , which represents the objective function, must equal .<br><br>First, let's prove that if a solution  is feasible then the intervals in  are pairwise-disjoint. Assume that  is a feasible solution and suppose that  was not pairwise-disjoint. This would imply that for some point , at least two of the intervals in  are also in . However, this would mean that  which, since it was shown that , also implies that  which breaks one of the constraints of the ILP and thus contradicts the assumption that  is a feasible solution. Therefore, by contradiction, it must be true that if  is a feasible solution, then  is pairwise disjoint.<br>Now, let's prove that if the intervals in  are pairwise disjoint, then  is a feasible solution. Assume that  is pairwise disjoint and suppose that  was not a feasible solution. If  is not a feasible solution, then we know that the statement  does not hold and that therefore, there must be some point  that exists in more than one of the intervals that were chosen to be in , implying that the elements of  are not pairwise disjoint and thus contradicting our assumption. It must, therefore, be true that  if the interval  is pairwise disjoint, then  is a feasible solution. <br><br> Let's first prove that if  is an optimal solution then  has maximum cardinality among all subsets of pairwise disjoint intervals in . Assume that  is an optimal solution and consider if  was not the subset of pairwise disjoint intervals with maximum cardinality. This implies that there exists a subset  such that all pairs in  satisfy the constraint that all its elements are disjoint from each other, and that . Since, if we were to represent all the elements of  as a solution to the ILP then that solution would be maximizing the objective function while ensuring that the constraints are met, it must be false that  was an optimal solution to begin with (since we know that  but that the objective function value of  would be higher). Therefore, it must be true that if  is an optimal solution then  has maximum cardinality among all subsets of pairwise disjoint intervals in .<br>Now let's prove that if  has maximum cardinality among all subsets of pairwise disjoint intervals in  then  is an optimal solution. Assume that  has maximum cardinality among all subsets of pairwise disjoint intervals and suppose that   is not an optimal solution. If  is not an optimal solution then we know that there exists a solution  such that the objective function value of  is greater than that of . In order for that to be true, we know that  must correspond to a set of intervals  such that there are more intervals in it than  which contradicts our assumption that  has maximum cardinality among all subsets of pairwise disjoint intervals. It must, therefore, be true that if  has maximum cardinality among all subsets of pairwise disjoint intervals in  then  is an optimal solution. <br><br>We assume that  is a feasible solution and that   is the leftmost right interval endpoint of all intervals whose corresponding variables have fractional values in .<br>Let . Since  is a fractional value, then it must be true that all other intervals  have a corresponding value in  that is either fractional or equal to zero. Since an interval that has a fractional value in  and contains  must also contain , and since any such interval gets its corresponding value in  set to zero while that of  gets set to , then it must also be true that . Since any interval that does not overlap with  has its corresponding value in  remain the same as it was , then it must be true that that  and . <br>Therefore, given that ,  must be feasible.<br><br>Given that both  and  are feasible, there exists some subset  such that, in the conversion between  and , . <br>Describe the objective function value of  as Since we have shown in (4) that all of the elements of  would have fractional or zero values in , we know that Since we have also shown in (4) that<br>
We can conclude that<br>
If we describe the objective function value of  as <br><br>Then we see that the following inequality must also be true <br><br>Which is equivalent to <br><br>In the mapping from  to , we select at least one variable  and set its value to 1, replacing its original fractional value. Any other variable  that is fractional and overlaps with the interval of  is set to zero, while the remaining variables are directly mapped to . As a result, the number of variables with fractional values in , , will always decrease by at least  in the mapping from  to , and therefore,  ]]></description><link>algorithms-ii-assignments/assignment-1-summative.html</link><guid isPermaLink="false">Algorithms II Assignments/Assignment 1 Summative.md</guid><pubDate>Fri, 04 Oct 2024 17:35:40 GMT</pubDate></item><item><title><![CDATA[Assignment 3 Summative]]></title><description><![CDATA[ 
 <br><br><br>Relating routes to vertices, edges to possible flight plans<br>We represent the input of the flight scheduling problem as a network where each route  is represented by a pair of vertices  corresponding to the departure and destination cities of  respectively, and which are connected to each other via a directed edge from  to . We draw a directed edge from  to  if  can be flown after  by the same plane.<br>Relating flow assignments to flight plans<br>If the flow assignment  on an edge  is , then the same plane visits the city represented by , followed by the city represented by . More specifically, if the flow  on the edge between two vertices  and  (representing the departure and destination cities of a route ) is equal to , then that route  is serviced by a plane. If there the flow  between the destination city of route  and the departure city of route  is equal to , then some plane services  followed by  (the routes are serviced by the same plane).<br>Assigning edge capacities and demands<br>We assign a capacity  for every edge  to ensure that no plane takes the same path twice. We also assign a demand  if the edge  is between two vertices corresponding to the same route ("between its departure and destination cities"), and  if it connects vertices of different routes. <br>Creating additional vertices to control the flow (number of planes)<br>We also  create three additional vertices  and  such that we can control the total flow ("total number of planes") in the graph. Flow can only be created or absorbed by  and . We draw a directed edge from  to , a directed edge from  to every departure () vertex in the network, and a directed edge from every destination () vertex in the network to . To control the total flow in our network, we set the capacity of  to be  and its demand to . We set the capacity to  and the demand to  for every other one of these new edges. <br><br>Constructing flow from solution to flight scheduling problem + showing it abides by defined edge capacities and demands<br>A solution to the flight scheduling problem consists of a set of subsets of routes that can all be flown by the same plane. Since the routes have to be flown in some sequence by their plane, we more specifically define a solution to the flight scheduling problem as a set of non-repetitive sequences   such that each sequence represents routes that can all be flown by the same plane in that sequence.<br>We thus know that for any two routes ,  that are placed consecutively in some sequence , we will have at most one plane, the same plane, servicing them and thus the flow of any edge  connecting two such routes will be <br><br>Since a solution ensures that every route will be serviced, we also know that for any edge  whose vertices correspond to the departure and destination vertices of the same route , the flow of that edge will be <br><br>Since we know that any two routes  that are not chosen to be in the same subset will not have any planes flying between them. We know that for any other edge  connecting the destination vertex of on of them to the departure vertex of the other the flow will be <br>
<br>Since we know that a solution to the flight scheduling problem utilizes at least one plane and at most  planes, we know that at most  flow will be "produced" by the source node , and therefore that Moreover, since we know that only one plane services each subset of route , we know that at most one unit flow will be assigned from  to any other vertex and therefore, for any edge  between  and the departure vertex of a route, the flow will be at most <br>
And therefore, we also know that at most one unit of flow will be travelling from a destination vertex  to , making the flow of such an edge at most <br>
<br>Since the above cases cover all possible edges in the network and their flow assignment, we know that the constraint   is met by any solution to the flight scheduling problem. <br>Proving that flow assignments can never be fractional in a feasible flow + summarizing constructed flow<br>Now, consider the case where any edge in the network is assigned a fractional flow. If this occurs, it would be impossible for that flow to feasibly reach the terminal vertex . This is because the flow must pass through at least one edge that connects a "departure" vertex to a "destination" vertex of some route. The capacity and demand of such edges are both defined as 1 (an integer). Therefore, assigning a fractional flow to these edges would violate feasibility. Thus, for the flow to be feasible, the amount of flow assigned to any edge along an -path must be integral. This allows us to further specify the flow from  to any vertex, from any vertex to , and from  to  <br><br>To summarize the flow assigned by a solution of the flight scheduling problem to the edges of the network we defined<br><br>Proving that flow is conserved<br>To prove flow conservation, consider the fact that we can construct an -path for each sequence  by following these steps:<br>
<br>Start with the edge from  to .
<br>Then, add the edge from  to the departure vertex of the first route in .
<br>For each route in , add the edge from its departure vertex to its destination vertex.
<br>If the sequence continues with another route, add the edge from the destination vertex of the current route to the departure vertex of the next route.
<br>Finally, add the edge from the destination vertex of the last route in  to .
<br>Since each sequence is serviced by only one plane, there will be exactly one unit of flow passing through each path. This implies that for all consecutive edges  and  in a path<br><br>Since the flow through any edges not belonging to such a path would be 0, satisfying the above constraint implies satisfying flow conservation for all vertices  in the network.<br>Finally, since there can be at exactly one unit of flow on any edge between  and another vertex , we can conclude that if  sends  units of flow to , there will be  distinct -paths, each carrying one unit of flow. Consequently, the terminal node  will receive exactly  units of flow, ensuring flow conservation by absorbing the incoming flow.<br>Conclusion<br>Therefore, by proving that the constructed flow conserves flow at every vertex an across the entire network, and abides by all edge capacities and demands, we have shown that we can construct a feasible flow from a solution to the flight scheduling problem. <br><br>Constructing a solution to the flight scheduling problem from a feasible flow<br>Given a feasible flow in our network, we want to construct at most  subsets of routes, where each subset can be flown by a single plane. We compute a set of vertex-disjoint -flows of our network, each of which correspond to a subset in a solution to the flight scheduling problem.<br>We iterate through the following steps to construct our solution<br>
<br>Pick a departure vertex  such that , we create a new set called , and add the route  to it. 
<br>Perform a depth first search starting at , adding each pair of departure and destination vertices, , that we encounter to the set  in the form of of the route they correspond to, , as long as the condition that the edge connecting our current vertex and  has a flow of 1.
<br>Mark all the vertices visited in this search, including , such that they are not visited again by future search. 
<br>These steps effectively compute -flows in the network as:<br>
<br>Any edge  will be preceded by the edge .
<br>We follow edges with a flow of , where any flow will eventually need to be absorbed by  (as there are no edges leading back to ). This implies that our search ends when we reach , and ensures that flow is conserved along the path discovered by our search.
<br>We claim that the set of "" sets that we found is effectively the set of of subsets of routes that represent a solution to the flight scheduling problem. To prove this claim, we need to prove that  the subsets are disjoint, that there are at most  of them, and that the routes added to each subset can be serviced by a a single plane.<br>Proving that the computed subsets are disjoint<br>To prove that the subsets are disjoint, we point to the fact that we mark the vertices encountered in each iteration (in which an iteration corresponds to the "filling" of a single subset) as visited, ensuring that they are note visited again by future iterations, and therefore, that the route corresponding to each pair of departure-destination vertices is not added to more than one subset.<br>Proving that there are at most  subsets<br>To prove that there are at most  such subsets, we recall the fact that the capacity of the edge  is  and that we follow the path taken by one unit of flow at each iteration. This implies that there would be at most  iterations and, since we create one subset per iteration, at most  subsets would be created.<br>Proving that the routes in each subset can all be serviced by the same plane<br>To prove that the routes in each subset can all be serviced by the same plane, we recall the fact that we constructed our network such that a directed edge is drawn from the destination vertex  of some route  to the departure vertex  of another route  only if  can be flown after . This means that if we are constructing each subset  by following the flow starting at some vertex  then we know that the next vertex added to  will correspond to a route that can be serviced after the route represented by  without breaking the laws of physics or neglecting servicing our plane. <br>Conclusion<br>Therefore, we have proven that by following the steps described above, we can construct a solution to the flight scheduling problem from a feasible flow of our network. <br><br><br>Note: we assume that  is represented as a complete graph with the capacity and demand of any non-existent edges being .<br>Constructing  from <br>Let  be the minimum value of an edge demand in . We can convert  to  by adding  to all edge capacities, demands, and flow assignments in <br><br>This ensures that for edges  with , their demand in  becomes , while edges with a higher demand are assigned a positive  value. This also ensures that all capacities in  are nonnegative in , as the minimum value of any edge capacity will always be greater than or equal to the minimum edge demand in . <br>To map a flow  in  to a flow  in , we add  to all the flow assignments in <br>
To map a flow  in  to a flow  in , we subtract  from all the flow assignments in <br>
<br>Proof that flow in  is feasible if corresponding flow in  is feasible<br>Assume that the flow  in  is feasible, then the following constraints hold<br><br>We aim to demonstrate that the following constraints hold in for the corresponding flow  in <br><br>To prove that the first constraint holds, we observe that<br>
Is equivalent to<br>
Which is the equivalent to adding  to all the terms of<br>
The fact that the above inequality holds true for all vertices  in   implies that<br>
To prove that the second constraint holds, we observe that<br>
Is equivalent to<br>
Which, given our assumption that the flow in  is feasible, evaluates to  for all , and thus implies that<br><br>Proof that flow in  is feasible if corresponding flow in  is feasible<br>Assume that the flow  in  is feasible, then the following constraints hold<br><br>We aim to demonstrate that the following constraints hold in for the corresponding flow  in <br><br>To prove that the first constraint holds, we observe that<br>
Is equivalent to<br>
Which is the equivalent to subtracting  from all the terms of<br>
<br>The fact that the above inequality holds true for all vertices  in   implies that<br>
To prove that the second constraint holds, we observe that<br>
Is equivalent to<br>
<br>Which, given our assumption that the flow in  is feasible, evaluates to  for all , and thus implies that<br><br>Proof that the mappings preserve the objective function value<br>Assume that  is a maximum flow of . We want to prove that if  is a maximum flow of  then its corresponding flow  in  is a maximum flow of  <br>Suppose, for the sake of contradiction, that there existed a flow  in  such that <br>If we map  to a flow  in , and map  back to <br>
Then we would be implying that there exists a flow  in  whose objective function value is greater than that of  which contradicts our assumption that  is a maximum flow of . <br>Therefore, if  is a maximum flow of  then its corresponding flow  in  is a maximum flow of .<br>Assume that  is a maximum flow of . We want to prove that if  is a maximum flow of  then its corresponding flow  in  is a maximum flow of  <br>Suppose, for the sake of contradiction, that there existed a flow  of  such that <br>If we map  to a flow  in , and map  to <br>
<br>Then we would be implying that there exists a flow  in  whose objective function value is greater than that of  which contradicts our assumption that  is a maximum flow of . <br>Therefore, if  is a maximum flow of  then its corresponding flow  in  is a maximum flow of .<br>By proving that  is a maximum flow of   its corresponding flow in , , is a maximum flow of , we have proven that the mappings we provided preserve the objective function value.<br><br>Note: We assume that  is represented as a complete network where any edge that doesn't exist in it has a flow, demand, and capacity of . This shouldn't affect the below arguments, but just for the sake of dealing with discrepancies in my notation ...<br>Since  is a network with nonnegative flow assignments<br>
Hence, despite the edges in  having no demand, the non-negativity of  enforces an implicit demand of  on all of them.<br>**Constructing feasible flow in  from feasible flow with value  in <br>To construct a feasible flow  in  from a feasible flow  with value  in , we need to assign a flow value  for all edges  (all edges that exist in ) that maintains the following conditions <br><br>Let the flow  for each edge  in  be defined as . Since we know that<br>
Then, since  is a subnetwork of , it must also be true that<br>
And therefore, it must also be true that<br>
Which, given that the capacity of any edge in  that also exists in  is defined as , is equivalent to<br>
Therefore<br>
Which is one of the conditions for a feasible flow in . <br>To show that flow is conserved, consider the following summation for all <br><br>Which can be expressed as <br><br>Which, given our definition of  for edges in , is equivalent to<br>
And therefore,<br>
Which proves that flow is conserved at each vertex  in  by . <br>To prove that flow is conserved overall across the network, we see that <br><br>We have therefore constructed a feasible flow  from  and shown that it is feasible.<br>Constructing a feasible flow  with value  in  from a feasible flow  in <br>We define the flow assignment of each edge in  as follows:<br><br>To demonstrate the feasibility of this construction, we first show that  for all . This is trivially true for edges described by cases 1, 3 since:<br>
<br>In case 1,  for all edges.
<br>In case 3, .
<br>For edges in case 2, the capacity in  is defined as . Since  is feasible in , we know that:<br><br>This implies:<br><br>Therefore, the constraint holds for all edges in case 2 as well.<br>Next, we need to verify that flow conservation holds and that the value of  is .<br>Assuming that  and  are the source/sink vertices of , we see that the value of  is <br>
And moreover, that this is the maximum flow of  as we set the value of all the out-edges of  to be the capacity of those edges. <br>Furthermore, we see that since the flow and demand of any edge  is set to , then for any  <br><br>Which, since we are assuming that flow is created and absorbed by  in ,  implies that the overall flow across the network is conserved.<br><br>We define the <a data-href="Residual Network" href="algorithms-ii-concepts/residual-network.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Network</a> in a network with nonnegative edge capacities and demands as<br>
Where the residual capacity of an edge  is<br>
If&nbsp;&nbsp;because&nbsp;, then this means that the edge&nbsp;&nbsp;is not used to its full capacity and we can move up to&nbsp;&nbsp;units of flow along this edge in an attempt to increase the flow from&nbsp;&nbsp;to&nbsp;.<br>If&nbsp;&nbsp;because&nbsp;, then this means that we can move up to&nbsp;&nbsp;units of flow from&nbsp;&nbsp;back to&nbsp;.<br>It is possible that the network&nbsp;contains both edges&nbsp;&nbsp;and&nbsp;. In this case, we can move up to&nbsp;&nbsp;additional units of flow from&nbsp;&nbsp;to&nbsp;&nbsp;along the edge&nbsp;, and we can move up to&nbsp;&nbsp;units of flow from&nbsp;&nbsp;to&nbsp;&nbsp;by pushing flow back along the edge&nbsp;. This is captured by the <a data-href="Residual Capacity" href="algorithms-ii-concepts/residual-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Capacity</a> of the edge&nbsp;&nbsp;in&nbsp;, which is the sum of these two quantities.<br>Given a feasible -flow, , in  an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a> is a path from  to  in the <a data-href="Residual Network" href="algorithms-ii-concepts/residual-network.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Network</a>  along which we could push more flow from  to . We determine how much flow we can push along each edge in , and therefore, how much flow we can push along such a path using the three cases described above. <br>We claim that to obtain a maximum flow in  by repeatedly finding augmenting paths in , pushing more flow along those paths, and then changing the flow assignments in  accordingly. We stop doing so once there are no more augmenting paths left. We call this procedure "adding" a maximum flow  in  to a pre-existing flow  in  to obtain a maximum flow of , . <br>To prove that this claim is true, assume that  and  are feasible, that  is a maximum flow in , and suppose for the sake of contradiction that there existed a maximum flow of , . The existence of  implies that there existed a path in , when  was assigned flow , along which we could have pushed more flow from  to . This is precisely the definition of an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a>, and therefore, the existence of  implies that there existed an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a> whose additional flow could have been added to  to make it bigger, and therefore, that  was not a maximum flow in , which contradicts our assumption that it is. Therefore, by contradiction, it must be true that "adding" maximum flow in   to  yields maximum flow in . ]]></description><link>algorithms-ii-assignments/assignment-3-summative.html</link><guid isPermaLink="false">Algorithms II Assignments/Assignment 3 Summative.md</guid><pubDate>Sun, 08 Dec 2024 22:38:25 GMT</pubDate></item><item><title><![CDATA[Assignment 4 Summative]]></title><description><![CDATA[ 
 <br><br>We run the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> algorithm on a network  with  vertices and  edges for at most  iterations. In each iteration, constructing the <a data-href="Residual Network" href="algorithms-ii-concepts/residual-network.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Network</a> , based on the flow  from the previous iteration, takes  time since we check each edge to determine whether it meets the conditions for inclusion in . To find an - path in , we perform a depth-first search (DFS) starting from , terminating when  is encountered, which takes  time. If an - path is found, the augmentation subroutine requires  time to find the minimum <a data-href="Residual Capacity" href="algorithms-ii-concepts/residual-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Capacity</a> by scanning the edges in the chosen -path and  to update the flow of every edge in the path. Consequently, the total runtime of the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> algorithm, without preprocessing, is .<br>However, if we preprocess the network by running DFS from  before the first iteration, identifying all vertices reachable from , and restricting the algorithm to only these vertices and edges, then each subsequent DFS in future iterations would take  time, as the network would already be known to be connected with  edges. The total running time of all the iterations thus becomes . Since this preprocessing step uses DFS, it takes  time and therefore, the total running time of the algorithm becomes . <br><br>To prove that each iteration of the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> algorithm strictly increases the objective function value of the flow, we note that in each iteration, an -path is identified in the current <a data-href="Residual Network" href="algorithms-ii-concepts/residual-network.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Network</a>. Along this path, the flow from  to  is augmented by the minimum <a data-href="Residual Capacity" href="algorithms-ii-concepts/residual-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Capacity</a> of the edges in the path. Since only the edges in this specific  path have their flow values updated, the additional flow introduced is not affected out by any other changes in the network. Consequently, the total flow is guaranteed to increase by a positive amount, strictly improving the value of the objective function in each iteration.<br>Assuming all edges  have integer capacities  bounded by some value , and since the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> initializes all the flow assignments in the network to  (an integral value), we know that the minimum <a data-href="Residual Capacity" href="algorithms-ii-concepts/residual-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Capacity</a><br>
By which the flow from  to  is augmented in each iteration, must also be an integer. Since the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> algorithm initializes the flow on all edges to , and in each iteration, it only updates the flow along the edges of the selected -path in the <a data-href="Residual Network" href="algorithms-ii-concepts/residual-network.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Network</a>, and since that update is defined as<br><br>For any two adjacent vertices  in the network, the flow remains an integer, as it results from adding or subtracting integer values in each iteration. <br>Since we have proven that the objective function value of the flow strictly increases with each iteration and that all edges in the network have integral flows, we can conclude that, in every iteration of the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> algorithm on a network with non-negative, integral edge capacities, the flow from  to  increases by at least .<br>Next, we establish bounds for the objective function value. The initial flow on all edges is set to  which satisfies flow conservation and is therefore feasible. Given that the flow increases at each iteration, the minimum value of the objective function is . The maximum flow that can be sent from  into the network is the sum of the capacities of all edges directed from  to a vertex that lies on an -path. Since the maximum flow is achieved when no flow is sent back to  while maintaining feasibility, the upper bound of the objective function value is equal to this sum.<br>
And therefore, given that the maximum capacity of any edge is , and since  has  out-edges (where  is the total number of vertices in the network), the upper bound on the objective function value is . <br>Since we established that the flow in this network increases by at least  in every iteration,<br>
and since the objective function will be at least  and at most , we conclude that the maximum number of iterations is achieved when the algorithm begins with the minimum objective function value, and then increases this value by  at each iteration to eventually obtain  as the final objective function value. And therefore, that the number of iterations is . <br>Given the bound we obtained in question 1, the final bound on the running time of the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> algorithm on the described network is  which is equivalent to <br><br>Description of Mapping Between a Network  with Rational Edge Capacities and  with Integer Edge Capacities<br>Given a network  with rational edge capacities, we want to convert these capacities to integer values in a new network . To do this, let  be the capacity of edge  in , expressed as a fraction:<br><br>where  is the numerator and  is the denominator of the capacity. The least common multiple (LCM) of all the denominators  across all edges is denoted as . <br>For each edge  in , we define the capacity in  as<br><br>The fact that  is a divisor of  ensures that  is an integer, and since<br><br>we can always recover  from  by dividing by . Thus, .<br>When we solve the flow problem in  (denoted by ), we convert the flows back to the original network  by dividing each flow by <br><br>Thus<br><br>Since  is computed using operations on the capacities in , which can be expressed as rational numbers with denominator , we conclude that  can also be written with a denominator , ensuring that  is an integer.<br>If  is Feasible, Then  is Feasible<br>Assume that the flow  in  is feasible. This implies<br><br>We want to show that the flow  in the original network , derived from  by dividing by , is also feasible. Since   flow conservation is preserved as follows<br><br>Now, for the capacity constraints, if , then<br><br>Which implies<br><br>This shows that  satisfies the capacity constraints in  and is thus feasible.<br>If  is Feasible, Then  is Feasible<br>Assume that  is feasible in . This implies<br><br>Now, since , we check that flow conservation is preserved in <br><br>For the capacity constraints, if , then<br><br>so<br><br>Thus,  is feasible in .<br>Maximum Flow in  Corresponds to Maximum Flow in <br>Let  be a maximum feasible flow of . Consider the objective function value of   Let  be the flow corresponding to the mapping of  into . Assume, for the sake of contradiction, that there existed feasible flow in ,  whose objective function value was greater than that of . This implies that  If we map each of  and  to , we get  Let this mapping of  into  be called . Since we have shown that the any mapping from  to  maintains feasibility, we can conclude that that the above expression implies that there exists a feasible flow of  whose objective function value is greater than that of  which contradicts our assumption that  is a maximum feasible flow of .<br><br>Given any edge  with capacity  in  and  in , we have established the relationship:<br>
Since  has exclusively integral capacities, this implies that  will always be a multiple of . <br>Since the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> algorithm initializes all flow assignments in  to  (an integral value), and since the minimum <a data-href="Residual Capacity" href="algorithms-ii-concepts/residual-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Capacity</a> by which the flow from  to  is augmented by in each iteration over  is defined as<br>
 We see that  is also always a multiple of .<br>The <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> initializes all the flow assignments of  to , and at each iteration, changes the flow along the selected -path in the <a data-href="Residual Network" href="algorithms-ii-concepts/residual-network.html" class="internal-link" target="_self" rel="noopener nofollow">Residual Network</a> by adjusting the flow on each edge on that path as follows <br><br>Which implies that the flow of any edge  in  will also always be a multiple of , since each flow reassignment involves adding or subtracting multiples of .<br>From Question 2, we know that each iteration of the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> algorithm strictly increases the value of the objective function in a network with nonnegative capacities, regardless of integrality. Therefore, the objective function value in  increases by at least  in each iteration.<br>Bounding the Objective Function<br>Initially, the flow on all edges is set to , which is a feasible flow due to flow conservation. The minimum value of the objective function is therefore .<br>The maximum flow that can be sent from  into the network is bounded by the sum of the capacities of all edges directed from  to vertices on an -path. The maximum value of the objective function is achieved when no flow is sent back to  while maintaining feasibility. Thus, the upper bound on the objective function is the sum of these capacities. Since the maximum capacity of an edge in  is , and  has at most  out-edges (where  is the total number of vertices in the network), the upper bound on the objective function value is .<br>Thus, the objective function value for  will lie between  and .<br>Number of Iterations<br>Since the objective function starts at  and increases by at least  in each iteration, the maximum number of iterations is reached when the value of the objective function increases from  to . This means the number of iterations is:<br>
<br>Final Bound on Running Time<br>Combining this result with the general bound obtained in Question 1, we conclude that the total running time of the <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a> algorithm on the described network is<br>
(where  is the total number of edges in the network).<br>Which is equivalent to<br>
]]></description><link>algorithms-ii-assignments/assignment-4-summative.html</link><guid isPermaLink="false">Algorithms II Assignments/Assignment 4 Summative.md</guid><pubDate>Sun, 08 Dec 2024 22:38:25 GMT</pubDate></item><item><title><![CDATA[Assignment 6 Summative]]></title><description><![CDATA[ 
 <br><br>The ILP of the <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> problem is<br><br><br>And its LP relaxation is <br><br>Consider the column vector  whose elements are the variables  for all . Let  be an  matrix, and define  and  as the -element column vectors filled with  and , respectively. We can express the above LP as follows:<br><br>Each column of  represents a vertex in  while each row of represents and edge. An entry of  is set to  if the vertex represented by its column is an endpoint of the edge represented by its row, and set to  otherwise. <br>The <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> of this LP is<br><br>For some -element column vector  filled with the set of <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> variables . <br><br>Let each edge in  correspond to an entry in . Given our previous definition of , each row of  represents a vertex in , while each column represents an edge. An entry of  is set to  if the vertex represented by its row is connected to the edge represented by its column. If we consider the ILP corresponding to the <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> LP:<br><br>the constraint  enforces that for each row of   (vertex in ), at most one of its non-zero entries (representing edges it's connected to in ) has a corresponding entry in  that is equal to . This ensures that no two edges selected in the solution share the same vertex. <br>An optimal solution of this ILP will then correspond to a set of edges such that no two selected edges share a common vertex and the number of selected edges is maximized. This is precisely the definition of a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> in . Therefore, an optimal solution of this ILP corresponds directly to a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> in the graph.  <br>While an optimal solution of the <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> LP allows for fractional values in , the set of edges connected to each vertex can still contribute at most  to the total objective function value, making the upper bound on the objective function value of an optimal solution to the <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> LP also an upper bound on the objective function of an optimal solution of its ILP. And consequently, the objective function value of an optimal solution to the <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> LP sets an upper bound on the size of a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of .<br><br>The <a data-href="Weak Duality" href="algorithms-ii-concepts/weak-duality.html" class="internal-link" target="_self" rel="noopener nofollow">Weak Duality</a> theorem tells us that if&nbsp;&nbsp;is a feasible solution of a primal minimization LP and&nbsp; is a feasible solution of its <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> maximization LP then<br>
Since we have associated minimizing the objective function of the primal LP we constructed with finding a minimum <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> and maximizing its <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> LP with finding a maximum valid <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>, this tells us that the number of vertices in a feasible <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> will always be   the number of edges in a feasible <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of a graph . And therefore, that the number of vertices in a minimum <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> will be   the number of edges in a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of a graph . <br><br>A maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of a graph  is a <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  such that no edges can be added to  without invalidating it. This definition implies that if there existed an edge , then at least one of the endpoints of  must be the endpoint of an edge that does exist in  (otherwise, there would be no reason for the addition of  to invalidate ). Therefore, if we were to take the set of endpoints of edges in , then each edge in the graph will have at least one endpoint in that set. Since such a set is precisely the definition of a <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a>, we can conclude that the set of endpoints of the edges in a maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> are a <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a>.<br><br>Since the set of endpoints of edges in a maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> are a <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a>, then for a maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  of a graph  and its corresponding <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> <br>
Since we explained in (4) that the size of a feasible <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> will always be  the size of a  feasible <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>, we know that for some maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  of <br>
Which thus implies that<br>
And therefore, we can conclude that the number of edges in a maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> is always at least half as much as the number of edges in a maximum one.<br><br>Every maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  of some graph  is a maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>. Otherwise, it would be possible to add more edges to , contradicting the fact that it's a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>. <br>Therefore, by the properties of a maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>, we can conclude that a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  of some graph  shares at least one endpoint with any maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  of the same graph. Since there are two endpoints per edge in any graph,  we can express this relationship as<br>
And therefore, the size of a maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  of any graph  is at least half the size of a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  of the same graph. ]]></description><link>algorithms-ii-assignments/assignment-6-summative.html</link><guid isPermaLink="false">Algorithms II Assignments/Assignment 6 Summative.md</guid><pubDate>Sun, 15 Dec 2024 00:26:50 GMT</pubDate></item><item><title><![CDATA[Assignment 7 Summative]]></title><description><![CDATA[ 
 <br><br><br>The algorithm initializes  to  and  to  then iteratively adds each vertex in  to either  or , effectively computing two non-empty subsets  and  where , which is precisely the definition of a cut over a graph . <br><br>I will use induction to proof that the algorithm computes a solution whose objective function value if  where  is the upper bound on . Let  denote the <br>Base case:<br>If , the algorithm stops after initializing  and , which forms the only possible cut, thus ensuring , and therefore,  must hold. <br>Inductive Step: <br>Assume  holds for all graphs with  vertices. Prove that  holds for all graphs with  vertices.<br>Given a graph , compute a cut  over  for an arbitrary vertex  (and let ). By the inductive step, , where  is the maximum -cut of  (with ). <br>Define  as the set of edges connected to  and note that , and therefore<br>
<br>To compute a cut  over , add  to either  or . Since the algorithm adds  to  if<br>
making . While adding  to  otherwise such that , we can state that Let the set of edges that this step of the algorithm adds to the cut be . Then Which implies<br>
And therefore<br>
And since ,<br>
Conclusion<br>Therefore, by induction, the weight of the cut computed by the given algorithm will always be  the upper bound of the maximum-weight cut of the graph. Since this implies that the algorithm will always compute a solution whose objective function value is at least  the optimal objective function value of the maximum cut problem, then the given algorithm must be a  <a data-href="Approximation" href="algorithms-ii-concepts/approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Approximation</a> of the maximum cut problem.<br><br><br>Claim:<br>Given a travelling salesman tour, , of a graph , we can obtain a spanning tree of <br>
 Where  is an arbitrarily-chosen edge in .<br>Proof: <br>To prove that  is a spanning tree, we need to prove that it has no cycle and that it is connected. <br>Acyclic: Since  is a cycle that visits each vertex at most once, then it cannot contain any sub-cycles, and therefore, removing any edges from it ensures that it is no longer a cycle, and therefore,  is acyclic. <br>Connected: Moreover, removing exactly one edge from  does not isolate any of its vertices or break its connectivity. Let the edge being removed be . Since  is a cycle, we know that  is connected to some vertex  and  is connected to some vertex  in it. Therefore,  and  would still have connections in .<br>Therefore,  is a spanning tree.<br>Since  is a spanning tree, we know that the weight of any minimum spanning tree of  will be  that of , and since , it will also be  that of . <br><br>The sum of all vertex degrees in any graph  twice the number of its edges:<br><br>Assume, for contradiction, that  has an odd number of odd-degree vertices. Since each of these vertices has an odd degree, their combined degree sum is odd. Vertices with even degrees, regardless of their count, contribute an even sum to the total. Thus, the overall sum of vertex degrees in  would be the sum of an odd and an even number, which is odd.<br>This contradicts the fact that the sum of all vertex degrees must be even (since it equals ). Therefore, , and by extension, any minimum spanning tree of , cannot have an odd number of odd-degree vertices.<br><br>We construct a Hamiltonian cycle  of  from  by either retaining an edge from  when it directly connects two vertices in , or replacing a sequence of edges in  that connects two vertices in  with a single edge that directly connects those vertices. This process ensures that every vertex in  is visited exactly once before returning to the starting vertex, thereby producing a Hamiltonian cycle of .<br><br>Claim: Given two vertices  that are  edges apart in  (in a chosen direction of traversal), the edge  has a weight of at most the sum of the weights of the edges connecting  and  in .<br>Proof:  <br>We will use induction on the number of edges  between  and  in , where  is measured in a chosen direction of traversal.<br>Base case:   <br>Let  be the vertex between  and  in the direction where they are two edges apart. By the triangle inequality, . Thus, the weight of  is at most the sum of the weights of the edges connecting  and  in , so the claim holds.<br>Inductive step:  <br>Assume the claim holds for  edges. Consider  and  separated by  edges in the chosen direction of traversal. Let  be the vertex  edges away from  in this direction. By the inductive hypothesis, the weight of  is at most the sum of the weights of the edges between  and . Applying the triangle inequality to , , and , we have:<br><br>Substituting the inductive hypothesis for , we find that  is at most the sum of the weights of the edges between  and , plus the weight of the edge , which is precisely the sum of the weights of the edges connecting  and  in .  <br>Thus, the claim holds for .<br>Note: Since the direction of traversal only affects the value of  and since the above proof is not actually dependent on the value of , this claim holds regardless of the chosen direction  of traversal. <br>Therefore, by induction, given any two vertices  that are some number of edges apart in , the edge  has a weight of at most the sum of the weights of the edges connecting  and  in . <br><br>This is relevant to the weight of  because it implies that replacing a sequence of edges in  by connecting two vertices in  with a single edge directly connecting them removes the summed weight of the sequence and introduces at most the same weight to . Since  is constructed by a combination of either this replacement or retaining an edge from  in , we can conclude that the total weight of  is at most the total weight of .<br><br>Claim: An even-length cycle  yields exactly two perfect matchings of its edges.<br>Proof:  <br>Since  has an even number of vertices, label the vertices consecutively as  in a chosen direction of traversal.  <br>First Perfect Matching: Pair each vertex with the next vertex in the cycle until every vertex is paired i.e., , , ..., . This forms a perfect matching, as each vertex is included in exactly one pair.  <br>Second Perfect Matching: Pair each vertex with the next vertex until every vertex is paired, but in the opposite direction of traversal. For example, . <br>Since each vertex can be matched with only one of the two vertices adjacent to it in the cycle, and since a matching cannot include edges whose endpoints overlap, these are the only two possible perfect matchings. Moreover, since every edge in the cycle has a nonnegative weight, one of these two perfect matchings must be the minimum perfect matching of the edges in the cycle.<br>Thus, the any even-length cycle yields exactly two perfect matchings, one of which is its minimum perfect matching.<br><br>The claim above proves that we can obtain a perfect matching  of  by picking the lighter perfect matching of  (which we know has an even number of vertices). <br><br>Claim: the weight of  is at most half the weight of <br>Proof: Since we pick half of the edges in  to be in , we can think of the weight of this matching as being one of two partitions of the weight of ,  and , where . Since we pick the lighter perfect matching of ,  is the partition whose weight represents the weight of . Since ,<br>
And since we have proven that<br>
We can conclude that<br>
<br><br>Since any minimum-weight <a data-href="Perfect matching" href="algorithms-ii-concepts/perfect-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Perfect matching</a>  of  will have at most the weight of , we can thus also claim that  has at most half the weight of , and since we proved that the weight of the travelling salesman tour of  is at most the weight of the travelling salesman tour of , we can further claim that  has at most half the weight of the travelling salesman tour of .<br><br> A vertex  occurs exactly once in the sequence defining , with the exception of the first and last positions of the sequence, which must belong to the same vertex. Since the first and last vertices of the sequence are the same,  must be a cycle. And since all the other vertices in  occur exactly once in , and since  is complete,  is a Hamiltonian cycle. <br><br>Building  by including the first occurrence of every vertex  in  tells us that the difference between the edges in  and  occurs when two vertices are consecutive in  but not in . Let such vertices be  and , by letting  and  be consecutive in , we are connecting them through the edge  rather the series of edges they were originally connected with in . As we proved in , doing so will always ensure that the weight of the edge  is at most the sum of the weights of the edges between  and  in  that we skip over. And therefore, the total weight of  will be at most that of . Since we have shown that the weight of  is at most the weight of the travelling salesman tour of  and that the weight of  is at most half of the weight of the travelling salesman tour of <br>Where  is the travelling salesman tour of . And therefore, <br>Which proves that  is no more than  times the weight of the travelling salesman tour of .]]></description><link>algorithms-ii-assignments/assignment-7-summative.html</link><guid isPermaLink="false">Algorithms II Assignments/Assignment 7 Summative.md</guid><pubDate>Sun, 15 Dec 2024 00:26:50 GMT</pubDate></item><item><title><![CDATA[Alternating Forest]]></title><description><![CDATA[ 
 <br>Every root is unmatched. every path from a root to its children is an alternating path.<br>We call a vertex even or odd depending on how many edges-away it is from its tree's root.<br>Given a set of unmatched vertices , we call an alternating forest maximum for  if  are the roots of the forest and every other vertex exists in the forest only if there exists an alternating path between itself and . <br><img alt="Pasted image 20241215013755.png" src="lib/media/pasted-image-20241215013755.png"><br>
(Norbert's work)]]></description><link>algorithms-ii-concepts/alternating-forest.html</link><guid isPermaLink="false">Algorithms II Concepts/Alternating Forest.md</guid><pubDate>Sun, 15 Dec 2024 05:40:59 GMT</pubDate><enclosure url="lib/media/pasted-image-20241215013755.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241215013755.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Approximation]]></title><description><![CDATA[ 
 <br>A -approximation algorithm for a some instance  of a problem with objective function  computes a solution  to that problem such that  where  is the objective function value of an optimal solution on  (assuming maximization here).<br>To prove that an algorithm is a -approximation, we show that  where  is some upper bound on  (again, assuming maximization). ]]></description><link>algorithms-ii-concepts/approximation.html</link><guid isPermaLink="false">Algorithms II Concepts/Approximation.md</guid><pubDate>Mon, 09 Dec 2024 21:44:32 GMT</pubDate></item><item><title><![CDATA[Augmenting Path]]></title><description><![CDATA[ 
 <br>Flows: if  is a feasible -flow in , and there exists a path from  to  such that , then  is an augmenting path. <br>Matchings: An path whose edges alternate between being in  and not being in , and whose endpoints are unmatched. ]]></description><link>algorithms-ii-concepts/augmenting-path.html</link><guid isPermaLink="false">Algorithms II Concepts/Augmenting Path.md</guid><pubDate>Sun, 15 Dec 2024 00:41:30 GMT</pubDate></item><item><title><![CDATA[Basic Feasible Solution]]></title><description><![CDATA[ 
 <br>Given an LP in <a data-href="Standard form" href="algorithms-ii-concepts/standard-form.html" class="internal-link" target="_self" rel="noopener nofollow">Standard form</a>, its BFS is the solution where only basic variables are allowed to be non-zero. This solution has a basis  such that <br><img alt="Pasted image 20241214171632.png" src="lib/media/pasted-image-20241214171632.png">]]></description><link>algorithms-ii-concepts/basic-feasible-solution.html</link><guid isPermaLink="false">Algorithms II Concepts/Basic Feasible Solution.md</guid><pubDate>Sun, 15 Dec 2024 00:06:10 GMT</pubDate><enclosure url="lib/media/pasted-image-20241214171632.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241214171632.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bipartite]]></title><description><![CDATA[ 
 <br>A graph whose vertices can be partitioned into two sets such that every edge has one endpoint in one set and the other in the other.<br>It has no odd-length cycles. ]]></description><link>algorithms-ii-concepts/bipartite.html</link><guid isPermaLink="false">Algorithms II Concepts/Bipartite.md</guid><pubDate>Sun, 15 Dec 2024 00:26:29 GMT</pubDate></item><item><title><![CDATA[Branching]]></title><description><![CDATA[ 
 <br>Branching and <a data-href="Kernelization" href="algorithms-ii-concepts/kernelization.html" class="internal-link" target="_self" rel="noopener nofollow">Kernelization</a>: a problem can have a very efficient branching algorithm but not a small kernel, but a small kernel without an efficient branching algorithm is not very useful.<br>The main idea behind branching is finding an elementary choice that we make in constructing a solution to the problem. Earlier choices influence later choices, and we just have to argue that we don't miss any optimal solutions.<br>\textbf{(<a data-href="Branching vector" href="algorithms-ii-concepts/branching-vector.html" class="internal-link" target="_self" rel="noopener nofollow">Branching vector</a>)} if an input of size  makes  choices, each bounded by , then the <a data-href="Branching vector" href="algorithms-ii-concepts/branching-vector.html" class="internal-link" target="_self" rel="noopener nofollow">Branching vector</a> is .<br>A branching algorithm makes  recsurive calls where  is the maximum of the <a data-href="Branching vector" href="algorithms-ii-concepts/branching-vector.html" class="internal-link" target="_self" rel="noopener nofollow">Branching vector</a> and  is the smallest real root of  ( is called the \textbf{<a data-href="Branching number" href="algorithms-ii-concepts/branching-number.html" class="internal-link" target="_self" rel="noopener nofollow">Branching number</a>}).]]></description><link>algorithms-ii-concepts/branching.html</link><guid isPermaLink="false">Algorithms II Concepts/Branching.md</guid><pubDate>Mon, 16 Dec 2024 05:32:06 GMT</pubDate></item><item><title><![CDATA[Branching number]]></title><description><![CDATA[ 
 <br>A <a data-href="Branching" href="algorithms-ii-concepts/branching.html" class="internal-link" target="_self" rel="noopener nofollow">Branching</a> algorithm makes  recursive calls where  is the maximum of the <a data-href="Branching vector" href="algorithms-ii-concepts/branching-vector.html" class="internal-link" target="_self" rel="noopener nofollow">Branching vector</a> and  is the smallest real root of  ( is called the <a data-href="Branching" href="algorithms-ii-concepts/branching.html" class="internal-link" target="_self" rel="noopener nofollow">Branching</a> number).<br>Checkout the v<a data-href="Vertex Cover Branching" href="algorithms-ii-algorithms/vertex-cover-branching.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover Branching</a> algorithm for an example of its usage.]]></description><link>algorithms-ii-concepts/branching-number.html</link><guid isPermaLink="false">Algorithms II Concepts/Branching number.md</guid><pubDate>Mon, 16 Dec 2024 15:09:36 GMT</pubDate></item><item><title><![CDATA[Branching vector]]></title><description><![CDATA[ 
 <br>If an input of size  makes  choices, each bounded by , then the <a data-href="Branching" href="algorithms-ii-concepts/branching.html" class="internal-link" target="_self" rel="noopener nofollow">Branching</a> vector is .]]></description><link>algorithms-ii-concepts/branching-vector.html</link><guid isPermaLink="false">Algorithms II Concepts/Branching vector.md</guid><pubDate>Mon, 16 Dec 2024 05:32:06 GMT</pubDate></item><item><title><![CDATA[Canonical form]]></title><description><![CDATA[ 
 <br>
<br>Must be a maximization LP
<br>Must be nonnegative
<br>All constraints are upper-bound
<br><img alt="Pasted image 20241214170111.png" src="lib/media/pasted-image-20241214170111.png">]]></description><link>algorithms-ii-concepts/canonical-form.html</link><guid isPermaLink="false">Algorithms II Concepts/Canonical form.md</guid><pubDate>Sat, 14 Dec 2024 21:01:13 GMT</pubDate><enclosure url="lib/media/pasted-image-20241214170111.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241214170111.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Clique]]></title><description><![CDATA[ 
 <br>Every pair of vertices is connected by an edge. ]]></description><link>algorithms-ii-concepts/clique.html</link><guid isPermaLink="false">Algorithms II Concepts/Clique.md</guid><pubDate>Sat, 14 Dec 2024 01:10:55 GMT</pubDate></item><item><title><![CDATA[Cluster Editing]]></title><description><![CDATA[ 
 <br>Given an undirected graph , find the minimum number of edges that need to be added to or removed from it such that it becomes a <a data-href="Clique" href="algorithms-ii-concepts/clique.html" class="internal-link" target="_self" rel="noopener nofollow">Clique</a>. ]]></description><link>algorithms-ii-concepts/cluster-editing.html</link><guid isPermaLink="false">Algorithms II Concepts/Cluster Editing.md</guid><pubDate>Sat, 14 Dec 2024 18:36:36 GMT</pubDate></item><item><title><![CDATA[Combining Recursive Calls]]></title><description><![CDATA[ 
 <br>Recall the <a data-href="Cluster Editing Reduction" href="algorithms-ii-algorithms/cluster-editing-reduction.html" class="internal-link" target="_self" rel="noopener nofollow">Cluster Editing Reduction</a> algorithm,]]></description><link>algorithms-ii-concepts/combining-recursive-calls.html</link><guid isPermaLink="false">Algorithms II Concepts/Combining Recursive Calls.md</guid><pubDate>Mon, 16 Dec 2024 14:57:42 GMT</pubDate></item><item><title><![CDATA[Complementary Slackness]]></title><description><![CDATA[ 
 <br> A good example of applying this is the <a data-href="Bipartite Minimum-Weight Perfect Matching LP" href="algorithms-ii-algorithms/bipartite-minimum-weight-perfect-matching-lp.html" class="internal-link" target="_self" rel="noopener nofollow">Bipartite Minimum-Weight Perfect Matching LP</a> and its application in <a data-href="The Hungarian Algorithm" href="algorithms-ii-algorithms/the-hungarian-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">The Hungarian Algorithm</a>. <br>Consider a maximization LP<br><br>And its <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a><br><br>If  and  are feasible solutions, then they satisfy complementary slackness if<br><br>Every primal variable  is either  or its corresponding <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> is tight<br>
<br><br>Every <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> variable  is either  or its corresponding primal is tight<br>
<br>Claim:  and  are both optimal solutions  they both satisfy complementary slackness<br>Proof:<br>If  and  are optimal solutions, they are also feasible solutions, and therefore they satisfy<br>
<br>Note that <br><br>So, we can plug this into the LP Duality condition to obtain<br>
And since <br>
By LP Duality, if they are both optimal solutions, then<br>
So, it must be true that<br>
<br>From the above, we can derive<br>
<br>Since  is optimal, it must satisfy a non-negativity constraint. The same applies for  because, as shown above .<br>Therefore, either  or , which is precisely what complementary slackness enforces. We can make an analogous argument for the  term.]]></description><link>algorithms-ii-concepts/complementary-slackness.html</link><guid isPermaLink="false">Algorithms II Concepts/Complementary Slackness.md</guid><pubDate>Sun, 15 Dec 2024 22:11:50 GMT</pubDate></item><item><title><![CDATA[Complete Bipartite Graph]]></title><description><![CDATA[ 
 <br>There exists an edge between every unique pair  and . This implies that a <a data-href="Perfect matching" href="algorithms-ii-concepts/perfect-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Perfect matching</a> will always exist.]]></description><link>algorithms-ii-concepts/complete-bipartite-graph.html</link><guid isPermaLink="false">Algorithms II Concepts/Complete Bipartite Graph.md</guid><pubDate>Sun, 15 Dec 2024 18:06:04 GMT</pubDate></item><item><title><![CDATA[Crown]]></title><description><![CDATA[ 
 <br>Two sets of vertices, .  is an independent set and every neighbour of some vertex  is in . <br><img alt="Pasted image 20241214150115.png" src="lib/media/pasted-image-20241214150115.png"><br>The <a data-href="Bipartite" href="algorithms-ii-concepts/bipartite.html" class="internal-link" target="_self" rel="noopener nofollow">Bipartite</a> graph between  and  (which possibly excludes edges between vertices in ), has a <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of size . <br>(1) The <a data-href="Bipartite" href="algorithms-ii-concepts/bipartite.html" class="internal-link" target="_self" rel="noopener nofollow">Bipartite</a> graph has a <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of size <br>
<img alt="Pasted image 20241214150308.png" src="lib/media/pasted-image-20241214150308.png"><br>
Therefore, no vertex in  is unmatched, which means that the size of the <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> is exactly the number of vertices in .<br>(2) If  is a <a data-href="Bipartite" href="algorithms-ii-concepts/bipartite.html" class="internal-link" target="_self" rel="noopener nofollow">Bipartite</a> graph, then it either has a maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>  that matches every vertex in  or a crown. Determining this takes  time.<br>We show that, using  the Hopcroft-Karp algorithm, we can compute the maximum <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of a graph in  time. Once we compute this <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a>, we can check if every vertex in it belongs to  in  time. If they don't, then we can run <a data-href="Alternating BFS" href="algorithms-ii-algorithms/alternating-bfs.html" class="internal-link" target="_self" rel="noopener nofollow">Alternating BFS</a> on it so that we can find  and . ]]></description><link>algorithms-ii-concepts/crown.html</link><guid isPermaLink="false">Algorithms II Concepts/Crown.md</guid><pubDate>Sun, 15 Dec 2024 18:06:04 GMT</pubDate><enclosure url="lib/media/pasted-image-20241214150115.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241214150115.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Depth-Bounded Search]]></title><description><![CDATA[ 
 <br>Combining the ideas of <a data-href="Parametrized" href="algorithms-ii-concepts/parametrized.html" class="internal-link" target="_self" rel="noopener nofollow">Parametrized</a> algorithms and <a data-href="Branching" href="algorithms-ii-concepts/branching.html" class="internal-link" target="_self" rel="noopener nofollow">Branching</a>, we make sure that each recursive call has an instance  with  with  being the <a data-href="Branching vector" href="algorithms-ii-concepts/branching-vector.html" class="internal-link" target="_self" rel="noopener nofollow">Branching vector</a>.]]></description><link>algorithms-ii-concepts/depth-bounded-search.html</link><guid isPermaLink="false">Algorithms II Concepts/Depth-Bounded Search.md</guid><pubDate>Mon, 16 Dec 2024 17:06:36 GMT</pubDate></item><item><title><![CDATA[Derandomization]]></title><description><![CDATA[ 
 <br>We aim to replace random choices in a randomized algorithm with deterministic ones by ]]></description><link>algorithms-ii-concepts/derandomization.html</link><guid isPermaLink="false">Algorithms II Concepts/Derandomization.md</guid><pubDate>Mon, 16 Dec 2024 17:24:29 GMT</pubDate></item><item><title><![CDATA[Dual]]></title><description><![CDATA[ 
 <br>If  is a maximization problem, then its dual  is a minimization one and vice versa. <br>Example<br>Consider the following maximization LP <br><br>Its dual is<br><br><img alt="Pasted image 20241208154017.png" src="lib/media/pasted-image-20241208154017.png">]]></description><link>algorithms-ii-concepts/dual.html</link><guid isPermaLink="false">Algorithms II Concepts/Dual.md</guid><pubDate>Sun, 08 Dec 2024 19:45:00 GMT</pubDate><enclosure url="lib/media/pasted-image-20241208154017.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241208154017.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Dual Fitting]]></title><description><![CDATA[ 
 <br>Given an <a data-href="NP-Hard" href="algorithms-ii-concepts/np-hard.html" class="internal-link" target="_self" rel="noopener nofollow">NP-Hard</a> problem that can be expressed as an ILP, we find a feasible solution to the <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> of its LP relaxation. <br>Thanks to <a data-href="Weak Duality" href="algorithms-ii-concepts/weak-duality.html" class="internal-link" target="_self" rel="noopener nofollow">Weak Duality</a>, this feasible solution tells us that a feasible solution to the ILP will always be some factor  away from an optimal solution of the problem.]]></description><link>algorithms-ii-concepts/dual-fitting.html</link><guid isPermaLink="false">Algorithms II Concepts/Dual Fitting.md</guid><pubDate>Thu, 19 Dec 2024 22:30:29 GMT</pubDate></item><item><title><![CDATA[Dynamic Programming]]></title><description><![CDATA[ 
 <br>A very nice, very cool way to store the solutions of sub-problems that we already solved such that they can be used to solve sub-problems that we haven't solved yet. <br>This works great when the number of subproblems, and the amount of time taken to solve a subproblem, are both polynomial.<br>But what about <a data-href="NP-Hard" href="algorithms-ii-concepts/np-hard.html" class="internal-link" target="_self" rel="noopener nofollow">NP-Hard</a> problems? Applying DP might still help reduce the <a data-href="Exponential" href="algorithms-ii-concepts/exponential.html" class="internal-link" target="_self" rel="noopener nofollow">Exponential</a> time needed to solve the problem if there are many overlapping subproblems, but more specifically, applying DP through a parameterized algorithm could help us reign-in the amount of space as it would be exponentially dependent only on the <a data-href="Hardness parameter" href="algorithms-ii-concepts/hardness-parameter.html" class="internal-link" target="_self" rel="noopener nofollow">Hardness parameter</a>.]]></description><link>algorithms-ii-concepts/dynamic-programming.html</link><guid isPermaLink="false">Algorithms II Concepts/Dynamic Programming.md</guid><pubDate>Mon, 16 Dec 2024 04:33:17 GMT</pubDate></item><item><title><![CDATA[Exponential]]></title><description><![CDATA[ 
 <br>We write  to express an algorithm with running time  where  is an exponential function. <br>It is worthwhile to look for the fastest possible exponential-time algorithms that we can find for a given problem as the gap between the running time of one exponential algorithm and the other can often be bigger than the one between an exponential algorithm and a polynomial one. ]]></description><link>algorithms-ii-concepts/exponential.html</link><guid isPermaLink="false">Algorithms II Concepts/Exponential.md</guid><pubDate>Mon, 09 Dec 2024 23:09:24 GMT</pubDate></item><item><title><![CDATA[Extreme Point Solution]]></title><description><![CDATA[ 
 <br>An extreme point solution of an LP is one that exists in the "corner" of the feasible region. It is not necessarily possible to find it in polynomial time using the <a data-href="Simplex" href="algorithms-ii-algorithms/simplex.html" class="internal-link" target="_self" rel="noopener nofollow">Simplex</a> algorithm. But we can find it efficiently if we already have an arbitrary optimal solution. <br>It CANNOT be written as a linear combination of two feasible solutions.]]></description><link>algorithms-ii-concepts/extreme-point-solution.html</link><guid isPermaLink="false">Algorithms II Concepts/Extreme Point Solution.md</guid><pubDate>Mon, 16 Dec 2024 17:06:36 GMT</pubDate></item><item><title><![CDATA[Family of Tight Inputs]]></title><description><![CDATA[ 
 <br>A way for us to ensure that our analysis of the <a data-href="Approximation" href="algorithms-ii-concepts/approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Approximation</a> guarantee of an <a data-href="Approximation" href="algorithms-ii-concepts/approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Approximation</a> algorithm is tight.<br>Given an <a data-href="Approximation" href="algorithms-ii-concepts/approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Approximation</a> algorithm, does there exist an infinite family of inputs for which the algorithm cannot perform significantly better than the upper/lower bound proven?<br>Example: <a data-href="Vertex Cover 2-Approximation" href="algorithms-ii-algorithms/vertex-cover-2-approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover 2-Approximation</a> <br>Consider the set of complete <a data-href="Bipartite" href="algorithms-ii-concepts/bipartite.html" class="internal-link" target="_self" rel="noopener nofollow">Bipartite</a> graphs over  vertices, a maximal <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> of any such graph includes all  vertices, and therefore the algorithm computes a <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of size . However, a <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> of size  does exist as the graph is complete. ]]></description><link>algorithms-ii-concepts/family-of-tight-inputs.html</link><guid isPermaLink="false">Algorithms II Concepts/Family of Tight Inputs.md</guid><pubDate>Sun, 15 Dec 2024 00:26:50 GMT</pubDate></item><item><title><![CDATA[FPT]]></title><description><![CDATA[ 
 <br>A problem is fixed-parameter tractable if there exists in algorithm that solves a parameterized instance  of it in , <a data-href="Exponential" href="algorithms-ii-concepts/exponential.html" class="internal-link" target="_self" rel="noopener nofollow">Exponential</a> time where  is an upper bound on the <a data-href="Hardness parameter" href="algorithms-ii-concepts/hardness-parameter.html" class="internal-link" target="_self" rel="noopener nofollow">Hardness parameter</a> of the instance . ]]></description><link>algorithms-ii-concepts/fpt.html</link><guid isPermaLink="false">Algorithms II Concepts/FPT.md</guid><pubDate>Mon, 09 Dec 2024 23:49:07 GMT</pubDate></item><item><title><![CDATA[Half-integrality]]></title><description><![CDATA[ 
 <br>Some ILPs have an LP relaxation whose optimal solutions are all multiples of . We can exploit this fact and round the solutions to obtain -approximations of optimal solutions. ]]></description><link>algorithms-ii-concepts/half-integrality.html</link><guid isPermaLink="false">Algorithms II Concepts/Half-integrality.md</guid><pubDate>Mon, 16 Dec 2024 15:15:20 GMT</pubDate></item><item><title><![CDATA[Hardness parameter]]></title><description><![CDATA[ 
 <br>Some property of the input to the problem that is related to how "hard" or "easy" a solution is to compute on this input. ]]></description><link>algorithms-ii-concepts/hardness-parameter.html</link><guid isPermaLink="false">Algorithms II Concepts/Hardness parameter.md</guid><pubDate>Fri, 13 Dec 2024 23:36:13 GMT</pubDate></item><item><title><![CDATA[Kernelization]]></title><description><![CDATA[ 
 <br>Kernelization is a set of <a data-href="Reduction rule" href="algorithms-ii-concepts/reduction-rule.html" class="internal-link" target="_self" rel="noopener nofollow">Reduction rule</a>s such that: <br>
<br>


<br>
For some (monotonically non-decreasing) , if  is a fully-reduced instance and  then it is has to either be a yes or no instance.

<br>
Getting to a fully-reduced instance from a non-reduced instance  should take  time for some constant . 

<br>Assuming we can solve any instance  in  time, the running time of the parameterized algorithm given by the kernelization procedure is:<br>
<img alt="Pasted image 20241213201433.png" src="lib/media/pasted-image-20241213201433.png">]]></description><link>algorithms-ii-concepts/kernelization.html</link><guid isPermaLink="false">Algorithms II Concepts/Kernelization.md</guid><pubDate>Sat, 14 Dec 2024 00:16:29 GMT</pubDate><enclosure url="lib/media/pasted-image-20241213201433.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241213201433.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Layering]]></title><description><![CDATA[ 
 <br>An off-branch of DP, we break the problem into subproblems and just consider one at a time.<br>A good example of this is the <a data-href="Set Cover Layering" href="Set Cover Layering" class="internal-link" target="_self" rel="noopener nofollow">Set Cover Layering</a> algorithm.]]></description><link>algorithms-ii-concepts/layering.html</link><guid isPermaLink="false">Algorithms II Concepts/Layering.md</guid><pubDate>Tue, 10 Dec 2024 01:34:52 GMT</pubDate></item><item><title><![CDATA[Matching]]></title><description><![CDATA[<a class="tag" href="?query=tag:todo" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#todo</a> 
 <br>A subset of edges of a graph such that no two edges in it share an endpoint.<br><br>For two matchings  and ,  is a collection of alternating paths and cycles such that if there are  augmenting paths for  and  for  in  then<br>
 Any even length path and any even-length cycle (which is all the cycles) in  contains an equal amount of edges in  and .<br>
 An odd-length path is in   it is an <a data-href="Augmenting Path" href="algorithms-ii-concepts/augmenting-path.html" class="internal-link" target="_self" rel="noopener nofollow">Augmenting Path</a> for one of the two matchings.<br>
Consider an odd-length path in  that contains one more edge in .<br>
<img alt="Pasted image 20241214212900.png" src="lib/media/pasted-image-20241214212900.png"><br>
Therefore, all the odd-length paths that contain one more edge in  than  in  are augmenting paths of , and vice versa. <br>
<br><a href=".?query=tag:todo" class="tag" target="_blank" rel="noopener nofollow">#todo</a> final remark to show that this is complete
<br>As long as there exists an alternating path  in ,  is a matching whose size is greater than  by one<br>If  is not a matching, then there exists a vertex in it that is attached to two edges. Since  is a matching, at most one of these edges is in . <br><img alt="Pasted image 20241214220431.png" src="lib/media/pasted-image-20241214220431.png"><br>
By the thing we proved before this, . ]]></description><link>algorithms-ii-concepts/matching.html</link><guid isPermaLink="false">Algorithms II Concepts/Matching.md</guid><pubDate>Sun, 15 Dec 2024 18:06:04 GMT</pubDate><enclosure url="lib/media/pasted-image-20241214212900.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241214212900.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Max-flow Min-cut]]></title><description><![CDATA[ 
 <br>Based on <a data-href="Strong Duality" href="algorithms-ii-concepts/strong-duality.html" class="internal-link" target="_self" rel="noopener nofollow">Strong Duality</a>, the maximum -flow of a graph  has the same solution as the <a data-href="Minimum st-Cut" href="algorithms-ii-concepts/minimum-st-cut.html" class="internal-link" target="_self" rel="noopener nofollow">Minimum st-Cut</a> of . ]]></description><link>algorithms-ii-concepts/max-flow-min-cut.html</link><guid isPermaLink="false">Algorithms II Concepts/Max-flow Min-cut.md</guid><pubDate>Sun, 08 Dec 2024 22:06:43 GMT</pubDate></item><item><title><![CDATA[Maxflow Procedure]]></title><description><![CDATA[ 
 <br>Refer to <a data-href="Ford-Fulkerson" href="algorithms-ii-algorithms/ford-fulkerson.html" class="internal-link" target="_self" rel="noopener nofollow">Ford-Fulkerson</a>.]]></description><link>algorithms-ii-concepts/maxflow-procedure.html</link><guid isPermaLink="false">Algorithms II Concepts/Maxflow Procedure.md</guid><pubDate>Sun, 08 Dec 2024 21:13:05 GMT</pubDate></item><item><title><![CDATA[Minimum st-Cut]]></title><description><![CDATA[ 
 <br>The flow of an -cut is the total outflow from vertices  to vertices . A similar definition applies to the capacity of the cut.<br>A minimum -cut is one were the capacity is minimized.<br>Moreover, the flow across an -cut is the -flow of the network.]]></description><link>algorithms-ii-concepts/minimum-st-cut.html</link><guid isPermaLink="false">Algorithms II Concepts/Minimum st-Cut.md</guid><pubDate>Sun, 08 Dec 2024 22:06:08 GMT</pubDate></item><item><title><![CDATA[Multiway Vertex Cut]]></title><description><![CDATA[ 
 <br>Given an undirected graph and a set of terminals, we want to "cut-up" the graph such that there is at most one terminal per resulting component.<br><img alt="Pasted image 20241216124943.png" src="lib/media/pasted-image-20241216124943.png">]]></description><link>algorithms-ii-concepts/multiway-vertex-cut.html</link><guid isPermaLink="false">Algorithms II Concepts/Multiway Vertex Cut.md</guid><pubDate>Mon, 16 Dec 2024 17:06:49 GMT</pubDate><enclosure url="lib/media/pasted-image-20241216124943.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241216124943.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Multiway Vertex Cut Half-Integrality]]></title><description><![CDATA[ 
 <br>While it is not as simple as the <a data-href="Vertex Cover Half-Integrality" href="algorithms-ii-concepts/vertex-cover-half-integrality.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover Half-Integrality</a>, we can still find a half-integral solution for the <a data-href="Multiway Vertex Cut" href="algorithms-ii-concepts/multiway-vertex-cut.html" class="internal-link" target="_self" rel="noopener nofollow">Multiway Vertex Cut</a> problem.<br>Let  be an optimal fractional solution, and let  be the region of vertices whose solution is , reachable from terminal . Lastly, let  be the set of vertices reachable from vertices in .<br>Lastly lastly, let  be all the vertices that exist in exactly one boundary while  is all the vertices that exist in more than one boundary.<br><img alt="Pasted image 20241216125739.png" src="lib/media/pasted-image-20241216125739.png"><br>And define  as the solution<br><img alt="Pasted image 20241216125845.png" src="lib/media/pasted-image-20241216125845.png"><br>We claim that  is a feasible solution.<br>Proof: <br>When looking at the path  between two terminals, you will always encounter at least one vertex in , or two vertices in . <br>Therefore,<br>
<img alt="Pasted image 20241216130513.png" src="lib/media/pasted-image-20241216130513.png"><br>We also claim that it is an optimal solution. We shall prove this using <a data-href="Complementary Slackness" href="algorithms-ii-concepts/complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a>. <br>Primal:<br><img alt="Pasted image 20241216130706.png" src="lib/media/pasted-image-20241216130706.png"><br><a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a>:<br><img alt="Pasted image 20241216130746.png" src="lib/media/pasted-image-20241216130746.png"><br>Our <a data-href="Complementary Slackness" href="algorithms-ii-concepts/complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a> conditions are:<br>
<img alt="Pasted image 20241216130828.png" src="lib/media/pasted-image-20241216130828.png"><br>Observe that <br><img alt="Pasted image 20241216131101.png" src="lib/media/pasted-image-20241216131101.png">]]></description><link>algorithms-ii-concepts/multiway-vertex-cut-half-integrality.html</link><guid isPermaLink="false">Algorithms II Concepts/Multiway Vertex Cut Half-Integrality.md</guid><pubDate>Mon, 16 Dec 2024 18:10:51 GMT</pubDate><enclosure url="lib/media/pasted-image-20241216125739.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241216125739.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[NP-Hard]]></title><description><![CDATA[ 
 <br>A problem that other problems  NP can be reduced to.]]></description><link>algorithms-ii-concepts/np-hard.html</link><guid isPermaLink="false">Algorithms II Concepts/NP-Hard.md</guid><pubDate>Sun, 08 Dec 2024 19:23:31 GMT</pubDate></item><item><title><![CDATA[Parametrized]]></title><description><![CDATA[ 
 <br>Unlike a normal decision problem<br>
A parameterized decision problem<br>
is a language with instance<br>
Where  is the problem's parameter.<br>An parameterized problem is <a data-href="FPT" href="algorithms-ii-concepts/fpt.html" class="internal-link" target="_self" rel="noopener nofollow">FPT</a> if and only if it has a <a data-href="Problem kernel" href="algorithms-ii-concepts/problem-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Problem kernel</a>.<br>If a problem has a <a data-href="Kernelization" href="algorithms-ii-concepts/kernelization.html" class="internal-link" target="_self" rel="noopener nofollow">Kernelization</a> procedure, then it can be solved in<br>
 time. And<br>
If a problem is <a data-href="FPT" href="algorithms-ii-concepts/fpt.html" class="internal-link" target="_self" rel="noopener nofollow">FPT</a>, then we know that its running time is exponentially-dependent on some <a data-href="Hardness parameter" href="algorithms-ii-concepts/hardness-parameter.html" class="internal-link" target="_self" rel="noopener nofollow">Hardness parameter</a> . More specifically, that there exists some <a data-href="FPT" href="algorithms-ii-concepts/fpt.html" class="internal-link" target="_self" rel="noopener nofollow">FPT</a> algorithm for it whose running time is . We can get a <a data-href="Kernelization" href="algorithms-ii-concepts/kernelization.html" class="internal-link" target="_self" rel="noopener nofollow">Kernelization</a> algorithm from this by: <br><br>Run the first  steps of the algorithm. If it terminates within these steps, then, by the rules of <a data-href="Kernelization" href="algorithms-ii-concepts/kernelization.html" class="internal-link" target="_self" rel="noopener nofollow">Kernelization</a>,<br>
And therefore, we should be certain that it has either a yes or no instance. <br><br>If it does not, then we take  to be the kernel and since , we can state that  is a <a data-href="Problem kernel" href="algorithms-ii-concepts/problem-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Problem kernel</a> of size . ]]></description><link>algorithms-ii-concepts/parametrized.html</link><guid isPermaLink="false">Algorithms II Concepts/Parametrized.md</guid><pubDate>Sat, 14 Dec 2024 01:07:17 GMT</pubDate></item><item><title><![CDATA[Perfect matching]]></title><description><![CDATA[ 
 <br>A <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> is a perfect <a data-href="Matching" href="algorithms-ii-concepts/matching.html" class="internal-link" target="_self" rel="noopener nofollow">Matching</a> if every vertex in the graph is an endpoint of some edge in it. <br>It has size .  ]]></description><link>algorithms-ii-concepts/perfect-matching.html</link><guid isPermaLink="false">Algorithms II Concepts/Perfect matching.md</guid><pubDate>Sun, 15 Dec 2024 00:26:50 GMT</pubDate></item><item><title><![CDATA[Primal-Dual Schema]]></title><description><![CDATA[ 
 <br>For non <a data-href="NP-Hard" href="algorithms-ii-concepts/np-hard.html" class="internal-link" target="_self" rel="noopener nofollow">NP-Hard</a> problems: <br>An algorithm that solves a problem using the primal-<a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> schema maintains a primal solution of the problem's LP, ad a <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> solution of its corresponding <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a><br>
<br>The <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> solution is always feasible.
<br>The primal solution is always satisfies some optimality criterion, such as <a data-href="Complementary Slackness" href="algorithms-ii-concepts/complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a>.
<br>The algorithm iteratively updates the primal and <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a> solutions such that the primal becomes feasible while maintaining the above conditions.<br>Once the primal is feasible, both solutions must be optimal because they satisfy the optimality criterion (<a data-href="Complementary Slackness" href="algorithms-ii-concepts/complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a>).<br>For <a data-href="NP-Hard" href="algorithms-ii-concepts/np-hard.html" class="internal-link" target="_self" rel="noopener nofollow">NP-Hard</a> problems:<br>We follow the same steps, but instead of satisfying <a data-href="Complementary Slackness" href="algorithms-ii-concepts/complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a> (which would thus imply P = NP), they satisfy <a data-href="Relaxed Complementary Slackness" href="algorithms-ii-concepts/relaxed-complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Relaxed Complementary Slackness</a>, thus making the primal an -<a data-href="Approximation" href="algorithms-ii-concepts/approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Approximation</a>. ]]></description><link>algorithms-ii-concepts/primal-dual-schema.html</link><guid isPermaLink="false">Algorithms II Concepts/Primal-Dual Schema.md</guid><pubDate>Tue, 10 Dec 2024 01:56:42 GMT</pubDate></item><item><title><![CDATA[Problem kernel]]></title><description><![CDATA[ 
 <br>Assuming we applied <a data-href="Kernelization" href="algorithms-ii-concepts/kernelization.html" class="internal-link" target="_self" rel="noopener nofollow">Kernelization</a>, a problem kernel is the fully-reduced instance.]]></description><link>algorithms-ii-concepts/problem-kernel.html</link><guid isPermaLink="false">Algorithms II Concepts/Problem kernel.md</guid><pubDate>Sat, 14 Dec 2024 00:30:27 GMT</pubDate></item><item><title><![CDATA[Randomization]]></title><description><![CDATA[ 
 <br>While it may seem counterintuitive, it is often useful to make random choices in an algorithm as we can <br>
<br>Prove that the probability of failure is low 
<br>Make random choices easily and quickly.
<br>This gives rise to two types of randomization Algorithms<br><img alt="Pasted image 20241216131816.png" src="lib/media/pasted-image-20241216131816.png"><br>We can combine these two techniques into one as follows<br><img alt="Pasted image 20241216132055.png" src="lib/media/pasted-image-20241216132055.png"><br><img alt="Pasted image 20241216132149.png" src="lib/media/pasted-image-20241216132149.png">]]></description><link>algorithms-ii-concepts/randomization.html</link><guid isPermaLink="false">Algorithms II Concepts/Randomization.md</guid><pubDate>Mon, 16 Dec 2024 17:21:53 GMT</pubDate><enclosure url="lib/media/pasted-image-20241216131816.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241216131816.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Randomized Rounding]]></title><description><![CDATA[ 
 ]]></description><link>algorithms-ii-concepts/randomized-rounding.html</link><guid isPermaLink="false">Algorithms II Concepts/Randomized Rounding.md</guid><pubDate>Mon, 16 Dec 2024 17:26:29 GMT</pubDate></item><item><title><![CDATA[Reduction rule]]></title><description><![CDATA[ 
 <br>A reduction rule transforms an instance  of a parameterized problem into another instance  such that  is a yes-instance   is a yes-instance.]]></description><link>algorithms-ii-concepts/reduction-rule.html</link><guid isPermaLink="false">Algorithms II Concepts/Reduction rule.md</guid><pubDate>Fri, 13 Dec 2024 23:55:30 GMT</pubDate></item><item><title><![CDATA[Relaxed Complementary Slackness]]></title><description><![CDATA[ 
 <br>A relaxation of <a data-href="Complementary Slackness" href="algorithms-ii-concepts/complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a>, enforces (assuming the primal is a maximization problem)<br>-Relaxed <a data-href="Complementary Slackness" href="algorithms-ii-concepts/complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a><br><br>-Relaxed <a data-href="Complementary Slackness" href="algorithms-ii-concepts/complementary-slackness.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a> <br><br>This makes the primal solution an -<a data-href="Approximation" href="algorithms-ii-concepts/approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Approximation</a> of an optimal solution.<br>
]]></description><link>algorithms-ii-concepts/relaxed-complementary-slackness.html</link><guid isPermaLink="false">Algorithms II Concepts/Relaxed Complementary Slackness.md</guid><pubDate>Tue, 10 Dec 2024 01:56:41 GMT</pubDate></item><item><title><![CDATA[Residual Capacity]]></title><description><![CDATA[ 
 <br>The residual capacity of an edge  is the amount of capacity it has remaining  the amount of flow that is sent along its opposite edge<br>
]]></description><link>algorithms-ii-concepts/residual-capacity.html</link><guid isPermaLink="false">Algorithms II Concepts/Residual Capacity.md</guid><pubDate>Sun, 08 Dec 2024 21:08:21 GMT</pubDate></item><item><title><![CDATA[Residual Network]]></title><description><![CDATA[ 
 <br>The network consisting of either <br>
<br>The edge  existing and 
<br>The edge  not necessarily existing, but  existing and  being true.
<br>In other words, either the edge  is not used to its full capacity, or its opposite-direction edge having a non-zero flow.]]></description><link>algorithms-ii-concepts/residual-network.html</link><guid isPermaLink="false">Algorithms II Concepts/Residual Network.md</guid><pubDate>Sun, 08 Dec 2024 21:06:19 GMT</pubDate></item><item><title><![CDATA[Set Cover]]></title><description><![CDATA[ 
 <br>Problem: Given a universe  and a collection  of subsets such that , the set cover is a subset  of  such that . We may aim to find either:<br>
<br>A minimum-weight set cover where  is some weight value assigned to the set .
<br>A minimum-size set cover.
<br>We will mainly focus on the minimum-weight problem as the minimum-size problem is just a special case of it.]]></description><link>algorithms-ii-concepts/set-cover.html</link><guid isPermaLink="false">Algorithms II Concepts/Set Cover.md</guid><pubDate>Mon, 09 Dec 2024 21:05:52 GMT</pubDate></item><item><title><![CDATA[Standard form]]></title><description><![CDATA[ 
 <br>We introduce basic/slack variables to <a data-href="Canonical form" href="algorithms-ii-concepts/canonical-form.html" class="internal-link" target="_self" rel="noopener nofollow">Canonical form</a> and write<br><img alt="Pasted image 20241214170428.png" src="lib/media/pasted-image-20241214170428.png"><br>Canonical vs. standard form:<br>
<img alt="Pasted image 20241214170457.png" src="lib/media/pasted-image-20241214170457.png">]]></description><link>algorithms-ii-concepts/standard-form.html</link><guid isPermaLink="false">Algorithms II Concepts/Standard form.md</guid><pubDate>Sat, 14 Dec 2024 22:07:58 GMT</pubDate><enclosure url="lib/media/pasted-image-20241214170428.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241214170428.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Steiner Tree]]></title><description><![CDATA[ 
 <br>A subset of vertices  and a tree that connects them in minimum weight.<br>The trick here is that we are allowed but not required to include vertices that are in . Therefore, the Steiner tree problem is <a data-href="NP-Hard" href="algorithms-ii-concepts/np-hard.html" class="internal-link" target="_self" rel="noopener nofollow">NP-Hard</a>. ]]></description><link>algorithms-ii-concepts/steiner-tree.html</link><guid isPermaLink="false">Algorithms II Concepts/Steiner Tree.md</guid><pubDate>Mon, 16 Dec 2024 04:33:17 GMT</pubDate></item><item><title><![CDATA[Strong Duality]]></title><description><![CDATA[ 
 <br>An optimal solution of  has an objective function value that equals that of an optimal solution of , its <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a>.]]></description><link>algorithms-ii-concepts/strong-duality.html</link><guid isPermaLink="false">Algorithms II Concepts/Strong Duality.md</guid><pubDate>Sun, 08 Dec 2024 20:05:12 GMT</pubDate></item><item><title><![CDATA[Vertex Cover]]></title><description><![CDATA[ 
 <br>Given a graph , find a vertex cover of either minimum size or weight. This problem is <a data-href="NP-Hard" href="algorithms-ii-concepts/np-hard.html" class="internal-link" target="_self" rel="noopener nofollow">NP-Hard</a>.]]></description><link>algorithms-ii-concepts/vertex-cover.html</link><guid isPermaLink="false">Algorithms II Concepts/Vertex Cover.md</guid><pubDate>Mon, 09 Dec 2024 21:44:14 GMT</pubDate></item><item><title><![CDATA[Vertex Cover Half-Integrality]]></title><description><![CDATA[ 
 <br>Claim: Any extreme-point solution of the LP relaxation of <a data-href="Vertex Cover" href="algorithms-ii-concepts/vertex-cover.html" class="internal-link" target="_self" rel="noopener nofollow">Vertex Cover</a> is half-integral.<br>
Proof: Assume  is a feasible solution with  being NOT half-integral.<br>Let  be the minimum between , the difference in magnitude between  and , and that between it and . <br>Now, define<br>
<img alt="Pasted image 20241216123551.png" src="lib/media/pasted-image-20241216123551.png"><br>We claim that  and  are feasible solutions of the LP relaxation.<br>
Proof:  is a feasible solution<br>By the definition of , . <br>We want to show that . <br><img alt="Pasted image 20241216124107.png" src="lib/media/pasted-image-20241216124107.png">]]></description><link>algorithms-ii-concepts/vertex-cover-half-integrality.html</link><guid isPermaLink="false">Algorithms II Concepts/Vertex Cover Half-Integrality.md</guid><pubDate>Mon, 16 Dec 2024 17:06:36 GMT</pubDate><enclosure url="lib/media/pasted-image-20241216123551.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241216123551.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Weak Duality]]></title><description><![CDATA[ 
 <br>Any feasible solution of a maximization problem  has an objective function value that is  the objective function value of a feasible solution of , its <a data-href="Dual" href="algorithms-ii-concepts/dual.html" class="internal-link" target="_self" rel="noopener nofollow">Dual</a>.]]></description><link>algorithms-ii-concepts/weak-duality.html</link><guid isPermaLink="false">Algorithms II Concepts/Weak Duality.md</guid><pubDate>Sun, 08 Dec 2024 20:05:03 GMT</pubDate></item></channel></rss>